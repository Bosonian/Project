{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pupil & Iris Segmentation Model Training\n",
    "\n",
    "Trains a lightweight U-Net segmentation model for pupil and iris detection\n",
    "using freely available open-source datasets.\n",
    "\n",
    "**Datasets used:**\n",
    "- OpenEDS (Facebook Research) - synthetic eye segmentation\n",
    "- LPW (Labelled Pupils in the Wild) - pupil center annotations\n",
    "- CASIA-Iris-Thousand - iris segmentation masks\n",
    "- Synthetic augmentation for anisocoria simulation\n",
    "\n",
    "**Output:** TFLite model + SavedModel for Cloud Run deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow opencv-python-headless albumentations gdown kaggle pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import json\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Download & Preparation\n",
    "\n",
    "We use multiple open-source datasets and unify them into a common format:\n",
    "- Image: 256x256 RGB\n",
    "- Mask: 256x256 with classes {0: background, 1: iris, 2: pupil}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('/content/eye_data')\n",
    "IMAGES_DIR = DATA_DIR / 'images'\n",
    "MASKS_DIR = DATA_DIR / 'masks'\n",
    "IMG_SIZE = 256\n",
    "NUM_CLASSES = 3  # background, iris, pupil\n",
    "\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(MASKS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download OpenEDS dataset (Facebook Research - open for research use)\n",
    "# OpenEDS provides eye images with semantic segmentation labels\n",
    "# Classes: background, sclera, iris, pupil\n",
    "# If direct download is unavailable, we generate synthetic training data\n",
    "\n",
    "def download_openeds():\n",
    "    \"\"\"Attempt to download OpenEDS or fall back to info message.\"\"\"\n",
    "    openeds_dir = DATA_DIR / 'openeds'\n",
    "    os.makedirs(openeds_dir, exist_ok=True)\n",
    "    \n",
    "    # OpenEDS requires sign-up at https://research.facebook.com/openeds-challenge\n",
    "    # If you have access, place the data in /content/eye_data/openeds/\n",
    "    print(\"OpenEDS dataset: Place downloaded files in\", openeds_dir)\n",
    "    print(\"Download from: https://research.facebook.com/openeds-challenge\")\n",
    "    print(\"If unavailable, synthetic data will be generated below.\")\n",
    "    return openeds_dir\n",
    "\n",
    "openeds_dir = download_openeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LPW (Labelled Pupils in the Wild) dataset\n",
    "# Provides pupil center coordinates which we convert to circular masks\n",
    "\n",
    "def download_lpw():\n",
    "    \"\"\"Download LPW dataset.\"\"\"\n",
    "    lpw_dir = DATA_DIR / 'lpw'\n",
    "    os.makedirs(lpw_dir, exist_ok=True)\n",
    "    \n",
    "    # LPW is available at: https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/labelled-pupils-in-the-wild-lpw\n",
    "    print(\"LPW dataset: Place downloaded files in\", lpw_dir)\n",
    "    print(\"Download from: https://perceptualui.org/research/datasets/LPW/\")\n",
    "    return lpw_dir\n",
    "\n",
    "lpw_dir = download_lpw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to download from Roboflow Universe - community eye segmentation datasets\n",
    "# These are freely available with API key\n",
    "\n",
    "def download_roboflow_datasets():\n",
    "    \"\"\"Download available eye segmentation datasets from Roboflow.\"\"\"\n",
    "    try:\n",
    "        from roboflow import Roboflow\n",
    "        # Pupil detection datasets on Roboflow Universe\n",
    "        # Users can get a free API key at https://roboflow.com\n",
    "        print(\"To use Roboflow datasets:\")\n",
    "        print(\"1. Sign up at https://roboflow.com (free tier)\")\n",
    "        print(\"2. Search 'pupil segmentation' or 'iris segmentation'\")\n",
    "        print(\"3. Export in 'Semantic Segmentation' format\")\n",
    "        print(\"4. Place in /content/eye_data/roboflow/\")\n",
    "    except ImportError:\n",
    "        print(\"Roboflow not installed. Using synthetic data generation.\")\n",
    "\n",
    "download_roboflow_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Data Generation\n",
    "\n",
    "To ensure we have sufficient training data regardless of dataset availability,\n",
    "we generate realistic synthetic eye images with precise ground-truth masks.\n",
    "\n",
    "This approach:\n",
    "- Creates eyes with varying pupil/iris ratios (simulating anisocoria)\n",
    "- Adds realistic lighting, reflections, and noise\n",
    "- Generates perfect pixel-level segmentation masks\n",
    "- Covers edge cases (dilated, constricted, off-center pupils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_eye(img_size=256, pupil_ratio=None, anisocoria=False):\n",
    "    \"\"\"\n",
    "    Generate a synthetic eye image with precise segmentation mask.\n",
    "    \n",
    "    Returns:\n",
    "        image: RGB uint8 array (img_size, img_size, 3)\n",
    "        mask: uint8 array (img_size, img_size) with values {0: bg, 1: iris, 2: pupil}\n",
    "    \"\"\"\n",
    "    img = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "    mask = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "    \n",
    "    center_x = img_size // 2 + np.random.randint(-15, 16)\n",
    "    center_y = img_size // 2 + np.random.randint(-15, 16)\n",
    "    center = (center_x, center_y)\n",
    "    \n",
    "    # Iris parameters\n",
    "    iris_radius = np.random.randint(img_size // 4, img_size // 3)\n",
    "    \n",
    "    # Iris color variations (brown, blue, green, hazel, gray)\n",
    "    iris_colors = [\n",
    "        (np.random.randint(40, 90), np.random.randint(60, 120), np.random.randint(100, 180)),   # brown\n",
    "        (np.random.randint(140, 200), np.random.randint(100, 160), np.random.randint(40, 80)),   # blue\n",
    "        (np.random.randint(60, 120), np.random.randint(120, 180), np.random.randint(60, 100)),   # green\n",
    "        (np.random.randint(80, 130), np.random.randint(100, 150), np.random.randint(120, 180)),  # hazel\n",
    "        (np.random.randint(120, 160), np.random.randint(120, 160), np.random.randint(120, 160)), # gray\n",
    "    ]\n",
    "    iris_color = iris_colors[np.random.randint(0, len(iris_colors))]\n",
    "    \n",
    "    # Pupil parameters\n",
    "    if pupil_ratio is None:\n",
    "        if anisocoria:\n",
    "            # Simulate pathological pupils\n",
    "            pupil_ratio = np.random.choice([\n",
    "                np.random.uniform(0.15, 0.25),  # very constricted\n",
    "                np.random.uniform(0.60, 0.80),  # very dilated\n",
    "            ])\n",
    "        else:\n",
    "            pupil_ratio = np.random.uniform(0.25, 0.65)  # normal range\n",
    "    \n",
    "    pupil_radius = int(iris_radius * pupil_ratio)\n",
    "    \n",
    "    # Slight pupil offset from iris center (realistic)\n",
    "    pupil_offset_x = np.random.randint(-3, 4)\n",
    "    pupil_offset_y = np.random.randint(-3, 4)\n",
    "    pupil_center = (center_x + pupil_offset_x, center_y + pupil_offset_y)\n",
    "    \n",
    "    # Draw sclera (white background around iris)\n",
    "    sclera_color = (\n",
    "        np.random.randint(220, 250),\n",
    "        np.random.randint(220, 250),\n",
    "        np.random.randint(225, 255)\n",
    "    )\n",
    "    # Elliptical sclera\n",
    "    sclera_w = int(iris_radius * np.random.uniform(1.6, 2.2))\n",
    "    sclera_h = int(iris_radius * np.random.uniform(1.1, 1.4))\n",
    "    cv2.ellipse(img, center, (sclera_w, sclera_h), 0, 0, 360, sclera_color, -1)\n",
    "    \n",
    "    # Draw iris with radial gradient\n",
    "    for r in range(iris_radius, 0, -1):\n",
    "        t = r / iris_radius  # 1.0 at edge, 0.0 at center\n",
    "        # Darker at edges (limbal ring)\n",
    "        darken = 0.5 + 0.5 * t  # outer is darker\n",
    "        c = tuple(int(v * darken) for v in iris_color)\n",
    "        cv2.circle(img, center, r, c, -1)\n",
    "    cv2.circle(mask, center, iris_radius, 1, -1)  # iris mask\n",
    "    \n",
    "    # Add iris texture (radial patterns)\n",
    "    num_fibers = np.random.randint(20, 50)\n",
    "    for _ in range(num_fibers):\n",
    "        angle = np.random.uniform(0, 2 * np.pi)\n",
    "        length = np.random.uniform(0.3, 0.95) * iris_radius\n",
    "        x1 = int(center_x + np.cos(angle) * pupil_radius * 1.1)\n",
    "        y1 = int(center_y + np.sin(angle) * pupil_radius * 1.1)\n",
    "        x2 = int(center_x + np.cos(angle) * length)\n",
    "        y2 = int(center_y + np.sin(angle) * length)\n",
    "        fiber_color = tuple(int(v * np.random.uniform(0.7, 1.3)) for v in iris_color)\n",
    "        fiber_color = tuple(max(0, min(255, v)) for v in fiber_color)\n",
    "        cv2.line(img, (x1, y1), (x2, y2), fiber_color, 1)\n",
    "    \n",
    "    # Draw pupil (black)\n",
    "    pupil_darkness = np.random.randint(5, 30)\n",
    "    cv2.circle(img, pupil_center, pupil_radius, (pupil_darkness, pupil_darkness, pupil_darkness), -1)\n",
    "    cv2.circle(mask, pupil_center, pupil_radius, 2, -1)  # pupil mask (overwrites iris)\n",
    "    \n",
    "    # Add specular reflection (corneal light reflex)\n",
    "    num_reflections = np.random.randint(1, 4)\n",
    "    for _ in range(num_reflections):\n",
    "        ref_x = center_x + np.random.randint(-iris_radius//2, iris_radius//2)\n",
    "        ref_y = center_y + np.random.randint(-iris_radius//2, iris_radius//2)\n",
    "        ref_r = np.random.randint(2, 8)\n",
    "        cv2.circle(img, (ref_x, ref_y), ref_r, (240, 240, 255), -1)\n",
    "    \n",
    "    # Add eyelids (partial occlusion)\n",
    "    if np.random.random() > 0.3:\n",
    "        lid_y_top = center_y - int(iris_radius * np.random.uniform(0.7, 1.2))\n",
    "        lid_y_bot = center_y + int(iris_radius * np.random.uniform(0.7, 1.2))\n",
    "        # Skin color\n",
    "        skin_color = (\n",
    "            np.random.randint(140, 220),\n",
    "            np.random.randint(120, 200),\n",
    "            np.random.randint(100, 180)\n",
    "        )\n",
    "        # Top eyelid\n",
    "        pts_top = np.array([\n",
    "            [0, 0], [img_size, 0], [img_size, lid_y_top],\n",
    "            [center_x, lid_y_top - np.random.randint(5, 25)],\n",
    "            [0, lid_y_top]\n",
    "        ])\n",
    "        cv2.fillPoly(img, [pts_top], skin_color)\n",
    "        cv2.fillPoly(mask, [pts_top], 0)  # background where lid is\n",
    "        \n",
    "        # Bottom eyelid\n",
    "        pts_bot = np.array([\n",
    "            [0, lid_y_bot], [center_x, lid_y_bot + np.random.randint(5, 25)],\n",
    "            [img_size, lid_y_bot], [img_size, img_size], [0, img_size]\n",
    "        ])\n",
    "        cv2.fillPoly(img, [pts_bot], skin_color)\n",
    "        cv2.fillPoly(mask, [pts_bot], 0)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, np.random.uniform(3, 12), img.shape).astype(np.int16)\n",
    "    img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Random brightness/contrast\n",
    "    alpha = np.random.uniform(0.7, 1.3)\n",
    "    beta = np.random.randint(-30, 31)\n",
    "    img = np.clip(alpha * img.astype(np.float32) + beta, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return img, mask\n",
    "\n",
    "\n",
    "# Test the generator\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(4):\n",
    "    aniso = i >= 2\n",
    "    img, msk = generate_synthetic_eye(IMG_SIZE, anisocoria=aniso)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"{'Anisocoria' if aniso else 'Normal'} eye\")\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(msk, cmap='viridis', vmin=0, vmax=2)\n",
    "    axes[1, i].set_title('Mask (0=bg, 1=iris, 2=pupil)')\n",
    "    axes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training dataset\n",
    "NUM_SYNTHETIC = 5000  # Generate 5000 synthetic samples\n",
    "NUM_ANISOCORIA = 2000  # Extra anisocoria cases for the clinical use case\n",
    "\n",
    "print(f\"Generating {NUM_SYNTHETIC + NUM_ANISOCORIA} synthetic eye images...\")\n",
    "\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Normal pupils\n",
    "for i in range(NUM_SYNTHETIC):\n",
    "    img, msk = generate_synthetic_eye(IMG_SIZE)\n",
    "    images.append(img)\n",
    "    masks.append(msk)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"  Normal: {i + 1}/{NUM_SYNTHETIC}\")\n",
    "\n",
    "# Anisocoria cases (extreme pupil sizes)\n",
    "for i in range(NUM_ANISOCORIA):\n",
    "    img, msk = generate_synthetic_eye(IMG_SIZE, anisocoria=True)\n",
    "    images.append(img)\n",
    "    masks.append(msk)\n",
    "    if (i + 1) % 500 == 0:\n",
    "        print(f\"  Anisocoria: {i + 1}/{NUM_ANISOCORIA}\")\n",
    "\n",
    "images = np.array(images, dtype=np.uint8)\n",
    "masks = np.array(masks, dtype=np.uint8)\n",
    "\n",
    "print(f\"\\nDataset shape: images={images.shape}, masks={masks.shape}\")\n",
    "print(f\"Mask classes: {np.unique(masks)}\")\n",
    "print(f\"Class distribution: bg={np.mean(masks==0):.1%}, iris={np.mean(masks==1):.1%}, pupil={np.mean(masks==2):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any real datasets that are available and merge with synthetic\n",
    "\n",
    "def load_real_datasets():\n",
    "    \"\"\"Load any downloaded real eye segmentation datasets.\"\"\"\n",
    "    real_images = []\n",
    "    real_masks = []\n",
    "    \n",
    "    # Check for OpenEDS data\n",
    "    openeds_imgs = sorted(glob.glob(str(DATA_DIR / 'openeds' / '**' / '*.png'), recursive=True))\n",
    "    if openeds_imgs:\n",
    "        print(f\"Found {len(openeds_imgs)} OpenEDS images\")\n",
    "        for img_path in openeds_imgs[:2000]:  # Limit to 2000\n",
    "            # OpenEDS has paired image/label files\n",
    "            mask_path = img_path.replace('/images/', '/labels/').replace('/image/', '/label/')\n",
    "            if os.path.exists(mask_path):\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                    msk = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    msk = cv2.resize(msk, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "                    # Remap OpenEDS classes to our format:\n",
    "                    # OpenEDS: 0=bg, 1=sclera, 2=iris, 3=pupil\n",
    "                    # Ours: 0=bg, 1=iris, 2=pupil\n",
    "                    new_msk = np.zeros_like(msk)\n",
    "                    new_msk[msk == 2] = 1  # iris\n",
    "                    new_msk[msk == 3] = 2  # pupil\n",
    "                    real_images.append(img)\n",
    "                    real_masks.append(new_msk)\n",
    "    \n",
    "    # Check for Roboflow exports\n",
    "    roboflow_imgs = sorted(glob.glob(str(DATA_DIR / 'roboflow' / '**' / '*.jpg'), recursive=True))\n",
    "    if roboflow_imgs:\n",
    "        print(f\"Found {len(roboflow_imgs)} Roboflow images\")\n",
    "        for img_path in roboflow_imgs[:2000]:\n",
    "            mask_path = img_path.replace('/images/', '/masks/').replace('.jpg', '.png')\n",
    "            if os.path.exists(mask_path):\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is not None:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                    msk = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    msk = cv2.resize(msk, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "                    real_images.append(img)\n",
    "                    real_masks.append(msk)\n",
    "    \n",
    "    if real_images:\n",
    "        print(f\"Loaded {len(real_images)} real images total\")\n",
    "        return np.array(real_images), np.array(real_masks)\n",
    "    else:\n",
    "        print(\"No real datasets found. Using synthetic data only.\")\n",
    "        return None, None\n",
    "\n",
    "real_imgs, real_msks = load_real_datasets()\n",
    "\n",
    "if real_imgs is not None:\n",
    "    images = np.concatenate([images, real_imgs], axis=0)\n",
    "    masks = np.concatenate([masks, real_msks], axis=0)\n",
    "    print(f\"Combined dataset: {images.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    images, masks, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "# Normalize images to [0, 1]\n",
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_val = X_val.astype(np.float32) / 255.0\n",
    "\n",
    "# One-hot encode masks\n",
    "y_train_oh = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_val_oh = tf.keras.utils.to_categorical(y_val, NUM_CLASSES)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train_oh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation pipeline using albumentations\n",
    "augment = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.7,\n",
    "                       border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=1),\n",
    "        A.MotionBlur(blur_limit=(3, 7), p=1),\n",
    "    ], p=0.3),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1),\n",
    "        A.CLAHE(clip_limit=4.0, p=1),\n",
    "    ], p=0.5),\n",
    "    A.GaussNoise(var_limit=(5, 30), p=0.3),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.4),\n",
    "])\n",
    "\n",
    "\n",
    "def create_tf_dataset(images, masks, batch_size=16, augment_fn=None, shuffle=True):\n",
    "    \"\"\"Create a tf.data.Dataset with optional augmentation.\"\"\"\n",
    "    def augment_sample(image, mask):\n",
    "        \"\"\"Apply augmentation to a single sample.\"\"\"\n",
    "        def _augment(img, msk):\n",
    "            img_np = (img.numpy() * 255).astype(np.uint8)\n",
    "            msk_np = np.argmax(msk.numpy(), axis=-1).astype(np.uint8)\n",
    "            result = augment_fn(image=img_np, mask=msk_np)\n",
    "            aug_img = result['image'].astype(np.float32) / 255.0\n",
    "            aug_msk = tf.keras.utils.to_categorical(result['mask'], NUM_CLASSES)\n",
    "            return aug_img.astype(np.float32), aug_msk.astype(np.float32)\n",
    "        \n",
    "        img_aug, msk_aug = tf.py_function(\n",
    "            _augment, [image, mask], [tf.float32, tf.float32]\n",
    "        )\n",
    "        img_aug.set_shape(image.shape)\n",
    "        msk_aug.set_shape(mask.shape)\n",
    "        return img_aug, msk_aug\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, masks))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=min(len(images), 2000))\n",
    "    if augment_fn is not None:\n",
    "        dataset = dataset.map(augment_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_ds = create_tf_dataset(X_train, y_train_oh, BATCH_SIZE, augment_fn=augment)\n",
    "val_ds = create_tf_dataset(X_val, y_val_oh, BATCH_SIZE, augment_fn=None, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
    "print(f\"Validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "Lightweight U-Net with MobileNetV3-Small encoder.\n",
    "- Optimized for mobile inference (~2MB model)\n",
    "- Fast enough for Cloud Run (<100ms per image)\n",
    "- 3-class output: background, iris, pupil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size=3):\n",
    "    \"\"\"Convolution + BatchNorm + ReLU block.\"\"\"\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_mobilenet_unet(input_shape=(256, 256, 3), num_classes=3):\n",
    "    \"\"\"\n",
    "    Build a U-Net with MobileNetV3-Small encoder.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: MobileNetV3-Small (pretrained on ImageNet)\n",
    "    - Decoder: Lightweight upsampling with skip connections\n",
    "    - Output: num_classes channel softmax\n",
    "    \"\"\"\n",
    "    # Encoder (MobileNetV3-Small)\n",
    "    base_model = tf.keras.applications.MobileNetV3Small(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        minimalistic=True,\n",
    "    )\n",
    "    \n",
    "    # Extract skip connection layers at different resolutions\n",
    "    # MobileNetV3-Small feature maps at various scales\n",
    "    layer_names = [\n",
    "        'multiply',          # 128x128, early features\n",
    "        'multiply_1',        # 64x64\n",
    "        'multiply_3',        # 32x32\n",
    "        'multiply_7',        # 16x16\n",
    "        'multiply_11',       # 8x8, deep features\n",
    "    ]\n",
    "    \n",
    "    # Try to find actual layer names (they may vary by TF version)\n",
    "    available_layers = [l.name for l in base_model.layers]\n",
    "    skip_layers = []\n",
    "    for name in layer_names:\n",
    "        if name in available_layers:\n",
    "            skip_layers.append(name)\n",
    "    \n",
    "    if len(skip_layers) < 3:\n",
    "        # Fallback: pick layers by output shape\n",
    "        print(\"Using fallback layer selection...\")\n",
    "        target_sizes = [128, 64, 32, 16, 8]\n",
    "        skip_layers = []\n",
    "        for target in target_sizes:\n",
    "            for layer in reversed(base_model.layers):\n",
    "                if hasattr(layer, 'output_shape'):\n",
    "                    shape = layer.output_shape\n",
    "                    if isinstance(shape, list):\n",
    "                        shape = shape[0]\n",
    "                    if len(shape) == 4 and shape[1] == target:\n",
    "                        if layer.name not in skip_layers:\n",
    "                            skip_layers.append(layer.name)\n",
    "                            break\n",
    "    \n",
    "    print(f\"Skip connection layers: {skip_layers}\")\n",
    "    \n",
    "    # Get encoder outputs at each scale\n",
    "    encoder_outputs = [base_model.get_layer(name).output for name in skip_layers]\n",
    "    \n",
    "    # Create encoder model\n",
    "    encoder = Model(inputs=base_model.input, outputs=encoder_outputs)\n",
    "    \n",
    "    # Freeze encoder initially (transfer learning)\n",
    "    encoder.trainable = False\n",
    "    \n",
    "    # Build decoder\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    skips = encoder(inputs)\n",
    "    \n",
    "    # Start from deepest features\n",
    "    x = skips[-1]\n",
    "    \n",
    "    # Decoder path with skip connections\n",
    "    decoder_filters = [128, 64, 48, 32, 24]\n",
    "    \n",
    "    for i in range(len(skips) - 2, -1, -1):\n",
    "        # Upsample\n",
    "        x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "        \n",
    "        # Resize if needed to match skip connection\n",
    "        skip = skips[i]\n",
    "        if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2]:\n",
    "            x = layers.Resizing(skip.shape[1], skip.shape[2])(x)\n",
    "        \n",
    "        # Concatenate skip connection\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "        \n",
    "        # Conv blocks\n",
    "        f = decoder_filters[min(i, len(decoder_filters) - 1)]\n",
    "        x = conv_block(x, f)\n",
    "        x = conv_block(x, f)\n",
    "    \n",
    "    # Final upsampling to input resolution\n",
    "    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "    if x.shape[1] != input_shape[0]:\n",
    "        x = layers.Resizing(input_shape[0], input_shape[1])(x)\n",
    "    x = conv_block(x, 16)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax', name='segmentation')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='pupil_iris_segnet')\n",
    "    return model, encoder\n",
    "\n",
    "\n",
    "model, encoder = build_mobilenet_unet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Dice coefficient for multi-class segmentation.\"\"\"\n",
    "    y_true_f = tf.cast(tf.reshape(y_true, [-1, NUM_CLASSES]), tf.float32)\n",
    "    y_pred_f = tf.cast(tf.reshape(y_pred, [-1, NUM_CLASSES]), tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n",
    "    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice[1:])  # Exclude background\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"Dice loss = 1 - dice coefficient.\"\"\"\n",
    "    return 1.0 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \"\"\"Combined cross-entropy + dice loss.\"\"\"\n",
    "    ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    ce = tf.reduce_mean(ce)\n",
    "    dl = dice_loss(y_true, y_pred)\n",
    "    return ce + dl\n",
    "\n",
    "\n",
    "def pupil_iou(y_true, y_pred):\n",
    "    \"\"\"IoU specifically for the pupil class.\"\"\"\n",
    "    y_true_pupil = y_true[..., 2]\n",
    "    y_pred_pupil = tf.cast(tf.argmax(y_pred, axis=-1) == 2, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true_pupil * y_pred_pupil)\n",
    "    union = tf.reduce_sum(y_true_pupil) + tf.reduce_sum(y_pred_pupil) - intersection\n",
    "    return (intersection + 1e-6) / (union + 1e-6)\n",
    "\n",
    "\n",
    "def iris_iou(y_true, y_pred):\n",
    "    \"\"\"IoU specifically for the iris class.\"\"\"\n",
    "    y_true_iris = y_true[..., 1]\n",
    "    y_pred_iris = tf.cast(tf.argmax(y_pred, axis=-1) == 1, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true_iris * y_pred_iris)\n",
    "    union = tf.reduce_sum(y_true_iris) + tf.reduce_sum(y_pred_iris) - intersection\n",
    "    return (intersection + 1e-6) / (union + 1e-6)\n",
    "\n",
    "\n",
    "print(\"Loss and metrics defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Two-phase training:\n",
    "1. **Phase 1**: Frozen encoder, train decoder only (10 epochs)\n",
    "2. **Phase 2**: Unfreeze encoder, fine-tune entire model with lower LR (20 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Train decoder only (encoder frozen)\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 1: Training decoder with frozen encoder\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=combined_loss,\n",
    "    metrics=[dice_coefficient, pupil_iou, iris_iou]\n",
    ")\n",
    "\n",
    "callbacks_p1 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_dice_coefficient', patience=5, mode='max', restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_dice_coefficient', factor=0.5, patience=3, mode='max', min_lr=1e-5\n",
    "    ),\n",
    "]\n",
    "\n",
    "history_p1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks_p1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tune entire model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Phase 2: Fine-tuning entire model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unfreeze encoder\n",
    "encoder.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=[dice_coefficient, pupil_iou, iris_iou]\n",
    ")\n",
    "\n",
    "callbacks_p2 = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_dice_coefficient', patience=7, mode='max', restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_dice_coefficient', factor=0.5, patience=3, mode='max', min_lr=1e-6\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        '/content/best_model.keras', monitor='val_dice_coefficient',\n",
    "        mode='max', save_best_only=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "history_p2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks_p2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_history(h1, h2):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Combine histories\n",
    "    metrics = ['loss', 'dice_coefficient', 'pupil_iou']\n",
    "    titles = ['Loss', 'Dice Coefficient', 'Pupil IoU']\n",
    "    \n",
    "    for ax, metric, title in zip(axes, metrics, titles):\n",
    "        train_vals = h1.history.get(metric, []) + h2.history.get(metric, [])\n",
    "        val_vals = h1.history.get(f'val_{metric}', []) + h2.history.get(f'val_{metric}', [])\n",
    "        epochs = range(1, len(train_vals) + 1)\n",
    "        \n",
    "        ax.plot(epochs, train_vals, 'b-', label='Train')\n",
    "        ax.plot(epochs, val_vals, 'r-', label='Validation')\n",
    "        ax.axvline(x=len(h1.history.get(metric, [])), color='g', linestyle='--', label='Unfreeze')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_p1, history_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "results = model.evaluate(val_ds)\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Loss: {results[0]:.4f}\")\n",
    "print(f\"  Dice: {results[1]:.4f}\")\n",
    "print(f\"  Pupil IoU: {results[2]:.4f}\")\n",
    "print(f\"  Iris IoU: {results[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "def visualize_predictions(model, images, masks, n=6):\n",
    "    \"\"\"Show input images, ground truth masks, and predictions side by side.\"\"\"\n",
    "    indices = np.random.choice(len(images), n, replace=False)\n",
    "    fig, axes = plt.subplots(n, 4, figsize=(16, 4 * n))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img = images[idx]\n",
    "        gt = np.argmax(masks[idx], axis=-1)\n",
    "        pred = model.predict(img[np.newaxis, ...], verbose=0)[0]\n",
    "        pred_class = np.argmax(pred, axis=-1)\n",
    "        \n",
    "        # Input image\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title('Input')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[i, 1].imshow(gt, cmap='viridis', vmin=0, vmax=2)\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[i, 2].imshow(pred_class, cmap='viridis', vmin=0, vmax=2)\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = img.copy()\n",
    "        overlay[pred_class == 1] = overlay[pred_class == 1] * 0.5 + np.array([0, 0.5, 0]) * 0.5\n",
    "        overlay[pred_class == 2] = overlay[pred_class == 2] * 0.5 + np.array([0.5, 0, 0]) * 0.5\n",
    "        axes[i, 3].imshow(np.clip(overlay, 0, 1))\n",
    "        axes[i, 3].set_title('Overlay (green=iris, red=pupil)')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, X_val, y_val_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circle fitting from segmentation mask - this is what the cloud service will use\n",
    "\n",
    "def fit_circle_from_mask(mask, class_id):\n",
    "    \"\"\"\n",
    "    Fit a circle to a segmentation mask region.\n",
    "    \n",
    "    Args:\n",
    "        mask: 2D array of predicted class IDs\n",
    "        class_id: which class to fit (1=iris, 2=pupil)\n",
    "    \n",
    "    Returns:\n",
    "        dict with {x, y, radius} in pixel coordinates, or None if not found\n",
    "    \"\"\"\n",
    "    binary = (mask == class_id).astype(np.uint8)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours:\n",
    "        return None\n",
    "    \n",
    "    # Get largest contour\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    if len(largest) < 5:\n",
    "        return None\n",
    "    \n",
    "    # Fit minimum enclosing circle\n",
    "    (cx, cy), radius = cv2.minEnclosingCircle(largest)\n",
    "    \n",
    "    # Also compute centroid for better center estimate\n",
    "    M = cv2.moments(largest)\n",
    "    if M['m00'] > 0:\n",
    "        cx = M['m10'] / M['m00']\n",
    "        cy = M['m01'] / M['m00']\n",
    "    \n",
    "    # Compute effective radius from area\n",
    "    area = cv2.contourArea(largest)\n",
    "    eff_radius = np.sqrt(area / np.pi)\n",
    "    \n",
    "    return {\n",
    "        'x': float(cx),\n",
    "        'y': float(cy),\n",
    "        'radius': float(eff_radius),\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_circles(model, image):\n",
    "    \"\"\"\n",
    "    Full inference pipeline: image -> segmentation -> circle fitting.\n",
    "    \n",
    "    Args:\n",
    "        model: trained segmentation model\n",
    "        image: RGB image (any size)\n",
    "    \n",
    "    Returns:\n",
    "        dict with pupil and iris circle parameters, scaled to original image\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Preprocess\n",
    "    resized = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(normalized[np.newaxis, ...], verbose=0)[0]\n",
    "    pred_mask = np.argmax(pred, axis=-1).astype(np.uint8)\n",
    "    \n",
    "    # Fit circles\n",
    "    pupil = fit_circle_from_mask(pred_mask, 2)\n",
    "    iris = fit_circle_from_mask(pred_mask, 1)\n",
    "    \n",
    "    # Scale back to original image coordinates\n",
    "    scale_x = w / IMG_SIZE\n",
    "    scale_y = h / IMG_SIZE\n",
    "    \n",
    "    if pupil:\n",
    "        pupil['x'] *= scale_x\n",
    "        pupil['y'] *= scale_y\n",
    "        pupil['radius'] *= (scale_x + scale_y) / 2\n",
    "    \n",
    "    if iris:\n",
    "        iris['x'] *= scale_x\n",
    "        iris['y'] *= scale_y\n",
    "        iris['radius'] *= (scale_x + scale_y) / 2\n",
    "    \n",
    "    # Compute confidence from mask probabilities\n",
    "    pupil_conf = float(np.mean(pred[..., 2][pred_mask == 2])) if np.any(pred_mask == 2) else 0.0\n",
    "    iris_conf = float(np.mean(pred[..., 1][pred_mask == 1])) if np.any(pred_mask == 1) else 0.0\n",
    "    \n",
    "    return {\n",
    "        'pupil': pupil,\n",
    "        'iris': iris,\n",
    "        'confidence': {\n",
    "            'pupil': round(pupil_conf, 3),\n",
    "            'iris': round(iris_conf, 3),\n",
    "        },\n",
    "        'ratio': round(pupil['radius'] / iris['radius'], 4) if pupil and iris else None,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the full pipeline on validation images\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(len(X_val))\n",
    "    result = predict_circles(model, (X_val[idx] * 255).astype(np.uint8))\n",
    "    print(f\"\\nSample {i+1}: {json.dumps(result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Models\n",
    "\n",
    "Export in two formats:\n",
    "1. **SavedModel** - for TensorFlow Serving on Cloud Run\n",
    "2. **TFLite** - optional client-side fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_DIR = Path('/content/exported_models')\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# 1. SavedModel for Cloud Run\n",
    "saved_model_path = EXPORT_DIR / 'pupil_segnet'\n",
    "model.save(str(saved_model_path))\n",
    "print(f\"SavedModel exported to: {saved_model_path}\")\n",
    "\n",
    "# Check size\n",
    "import subprocess\n",
    "size = subprocess.check_output(['du', '-sh', str(saved_model_path)]).decode().split()[0]\n",
    "print(f\"SavedModel size: {size}\")\n",
    "\n",
    "# 2. TFLite for potential client-side use\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = EXPORT_DIR / 'pupil_segnet.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TFLite model size: {len(tflite_model) / 1024 / 1024:.1f} MB\")\n",
    "print(f\"TFLite exported to: {tflite_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify exported SavedModel works\n",
    "loaded_model = tf.saved_model.load(str(saved_model_path))\n",
    "infer = loaded_model.signatures['serving_default']\n",
    "\n",
    "# Test inference\n",
    "test_img = X_val[0:1].astype(np.float32)\n",
    "test_input = tf.constant(test_img)\n",
    "output = infer(test_input)\n",
    "\n",
    "# Get output tensor name\n",
    "output_key = list(output.keys())[0]\n",
    "pred = output[output_key].numpy()\n",
    "print(f\"SavedModel input shape: {test_input.shape}\")\n",
    "print(f\"SavedModel output shape: {pred.shape}\")\n",
    "print(f\"SavedModel output key: {output_key}\")\n",
    "print(\"SavedModel verification: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TFLite model works\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f\"TFLite input: {input_details[0]['shape']}, dtype={input_details[0]['dtype']}\")\n",
    "print(f\"TFLite output: {output_details[0]['shape']}, dtype={output_details[0]['dtype']}\")\n",
    "\n",
    "# Run inference\n",
    "interpreter.set_tensor(input_details[0]['index'], test_img.astype(np.float32))\n",
    "interpreter.invoke()\n",
    "tflite_pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f\"TFLite prediction shape: {tflite_pred.shape}\")\n",
    "print(\"TFLite verification: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure inference speed\n",
    "import time\n",
    "\n",
    "# SavedModel speed\n",
    "times = []\n",
    "for _ in range(50):\n",
    "    start = time.time()\n",
    "    _ = model.predict(test_img, verbose=0)\n",
    "    times.append(time.time() - start)\n",
    "print(f\"SavedModel inference: {np.mean(times)*1000:.1f}ms (avg of 50 runs)\")\n",
    "\n",
    "# TFLite speed\n",
    "times = []\n",
    "for _ in range(50):\n",
    "    start = time.time()\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_img.astype(np.float32))\n",
    "    interpreter.invoke()\n",
    "    _ = interpreter.get_tensor(output_details[0]['index'])\n",
    "    times.append(time.time() - start)\n",
    "print(f\"TFLite inference: {np.mean(times)*1000:.1f}ms (avg of 50 runs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Models\n",
    "\n",
    "Download the exported models for Cloud Run deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package SavedModel for download\n",
    "!cd /content/exported_models && tar -czf /content/pupil_segnet_savedmodel.tar.gz pupil_segnet/\n",
    "print(\"Packaged SavedModel for download.\")\n",
    "\n",
    "# In Colab, use files.download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('/content/pupil_segnet_savedmodel.tar.gz')\n",
    "    files.download(str(tflite_path))\n",
    "    print(\"Downloads initiated.\")\n",
    "except ImportError:\n",
    "    print(\"Not in Colab. Models saved at:\")\n",
    "    print(f\"  SavedModel: /content/pupil_segnet_savedmodel.tar.gz\")\n",
    "    print(f\"  TFLite: {tflite_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model: `pupil_segnet`**\n",
    "- Architecture: U-Net with MobileNetV3-Small encoder\n",
    "- Input: 256x256 RGB image\n",
    "- Output: 256x256x3 softmax (background, iris, pupil)\n",
    "- Training: 7,000 synthetic + optional real data images\n",
    "- Two-phase training with transfer learning\n",
    "\n",
    "**Outputs:**\n",
    "1. `pupil_segnet/` - TF SavedModel for Cloud Run\n",
    "2. `pupil_segnet.tflite` - Quantized TFLite for optional client-side\n",
    "\n",
    "**Next steps:**\n",
    "1. Deploy SavedModel to Cloud Run (see `cloud/` directory)\n",
    "2. Add real datasets for fine-tuning when available\n",
    "3. Integrate cloud detection endpoint into PWA"
   ]
  }
 ]
}