{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PupilCheck - Pupil Segmentation Model Training\n",
    "\n",
    "Two-model approach for in-browser and cloud pupil/iris detection:\n",
    "1. **Pupil ROI Model** (~1-2MB TFLite) - Runs in-browser via TensorFlow.js\n",
    "2. **Full Image Model** (~10-15MB SavedModel) - Runs on Cloud Run\n",
    "\n",
    "**Real Datasets:**\n",
    "- MOBIUS (3,559 images with masks, RGB phone cameras)\n",
    "- iBUG Eye Segmentation (~2K images)\n",
    "- Roboflow pupilX (804 images)\n",
    "- Enhanced synthetic augmentation (15,000 images)\n",
    "\n",
    "**Total: ~20,000+ training samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (local Jupyter)\n",
    "!pip install -q tensorflow opencv-python-headless albumentations pillow scikit-learn matplotlib requests tqdm\n",
    "# Optional: pip install roboflow  (for Roboflow API download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os, json, glob, time, shutil, requests\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nimport albumentations as A\nfrom sklearn.model_selection import train_test_split\nimport matplotlib\nmatplotlib.use('Agg')  # headless rendering (no display needed)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# ---- Quick mode flag ----\n# Set QUICK_MODE = True for fast validation (~5 min on CPU)\n# Set QUICK_MODE = False for full training (~45-90 min on GPU)\nQUICK_MODE = os.environ.get('QUICK_MODE', 'false').lower() == 'true'\n\n# Configuration\nDATA_DIR = Path('./data')      # Relative to notebook\nMODELS_DIR = Path('./models')  # Export directory\nIMG_SIZE_FULL = 256            # Full image model input\nIMG_SIZE_ROI = 128             # ROI model input\nNUM_CLASSES = 3                # bg, iris, pupil\nBATCH_SIZE = 16 if not QUICK_MODE else 8\n\nfor d in [DATA_DIR, MODELS_DIR,\n          DATA_DIR / 'mobius', DATA_DIR / 'ibug',\n          DATA_DIR / 'roboflow', DATA_DIR / 'synthetic']:\n    d.mkdir(parents=True, exist_ok=True)\n\nprint(f\"TensorFlow: {tf.__version__}\")\nprint(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\nprint(f\"Quick mode: {QUICK_MODE}\")\nif QUICK_MODE:\n    print(\"  (500 synthetic samples, 3+3 epochs -- pipeline validation only)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## Dataset Download Instructions\n\nPlace datasets in the `data/` directory next to this notebook.  \nThe notebook works with **synthetic data only** if no real datasets are present -- you can train with just synthetic data and add real data later for improved accuracy.\n\n### MOBIUS (3,559 images, RGB phone cameras) -- RECOMMENDED\n1. Visit https://sclera.fri.uni-lj.si/datasets.html\n2. Fill out the download request form (requires academic/institutional email)\n3. You will receive a download link via email\n4. Extract into `data/mobius/`\nExpected structure: `data/mobius/` with parallel `images/` and `masks/` subdirectories (or any nested structure with matching image-mask pairs -- the loader auto-discovers them).\n\n### iBUG Eye Segmentation (~8,882 eye patches)\n1. Visit https://ibug.doc.ic.ac.uk/resources/ibug-eye-segmentation-dataset/\n2. Follow the download/request instructions on the page\n3. Extract into `data/ibug/`\nExpected structure: `data/ibug/images/` and `data/ibug/masks/`\n\n### Roboflow pupilX (804 images, CC BY 4.0)\n**Option A -- Roboflow Python SDK (recommended):**\n```bash\npip install roboflow\n```\n```python\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"YOUR_API_KEY\")  # Free signup at https://app.roboflow.com\nproject = rf.workspace(\"pupil-exnzt\").project(\"pupilx\")\ndataset = project.version(1).download(\"png-mask-semantic\", location=\"data/roboflow\")\n```\n**Option B -- Manual download:**\n1. Visit https://universe.roboflow.com/pupil-exnzt/pupilx\n2. Click \"Download Dataset\" and choose \"Semantic Segmentation\" format\n3. Extract into `data/roboflow/`\nExpected structure: `data/roboflow/train/`, `data/roboflow/valid/`, `data/roboflow/test/` each containing `images/` and `masks/` subdirs.\n\n### No datasets? No problem!\nIf none of the real datasets are available, the notebook will automatically generate 15,000 enhanced synthetic eye images and train on those. You can always add real data later and retrain for better accuracy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Shared utilities for all dataset loaders\n",
    "# ============================================================\n",
    "\n",
    "def find_mask_path(img_path):\n",
    "    \"\"\"Find the mask file corresponding to an image using common naming conventions.\"\"\"\n",
    "    img_path = str(img_path)\n",
    "    base = os.path.basename(img_path)\n",
    "    name_no_ext = os.path.splitext(base)[0]\n",
    "    parent = os.path.dirname(img_path)\n",
    "    grandparent = os.path.dirname(parent)\n",
    "\n",
    "    # Pattern 1: parallel directory (images/ -> masks/, labels/, annotations/)\n",
    "    for src_dir in ['images', 'image', 'imgs', 'img']:\n",
    "        if f'/{src_dir}/' in img_path or f'/{src_dir}' == img_path[-len(src_dir)-1:]:\n",
    "            for dst_dir in ['masks', 'mask', 'labels', 'label', 'annotations', 'segs']:\n",
    "                candidate = img_path.replace(f'/{src_dir}/', f'/{dst_dir}/')\n",
    "                for ext in ['.png', '.jpg', '.bmp', '.tif']:\n",
    "                    c = os.path.splitext(candidate)[0] + ext\n",
    "                    if os.path.exists(c):\n",
    "                        return c\n",
    "\n",
    "    # Pattern 2: same directory, mask suffix\n",
    "    for suffix in ['_mask', '_label', '_seg', '_gt', '.mask', '.label']:\n",
    "        for ext in ['.png', '.jpg', '.bmp']:\n",
    "            candidate = os.path.join(parent, name_no_ext + suffix + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                return candidate\n",
    "\n",
    "    # Pattern 3: sibling directory from grandparent\n",
    "    for subdir in ['masks', 'labels', 'annotations', 'groundtruth', 'gt']:\n",
    "        for ext in ['.png', '.jpg', '.bmp', '.tif']:\n",
    "            candidate = os.path.join(grandparent, subdir, name_no_ext + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                return candidate\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def remap_mask(mask):\n",
    "    \"\"\"Remap dataset-specific mask values to {0: bg, 1: iris, 2: pupil}.\"\"\"\n",
    "    unique = np.unique(mask)\n",
    "    new_mask = np.zeros_like(mask)\n",
    "\n",
    "    if len(unique) <= 4:\n",
    "        # Assume ascending order: bg=0, (sclera), iris, pupil\n",
    "        if len(unique) == 3:  # bg, iris, pupil\n",
    "            new_mask[mask == unique[1]] = 1  # iris\n",
    "            new_mask[mask == unique[2]] = 2  # pupil\n",
    "        elif len(unique) == 4:  # bg, sclera, iris, pupil\n",
    "            new_mask[mask == unique[2]] = 1  # iris\n",
    "            new_mask[mask == unique[3]] = 2  # pupil\n",
    "        elif len(unique) == 2:  # bg + one class (assume pupil)\n",
    "            new_mask[mask == unique[1]] = 2\n",
    "    else:\n",
    "        # Intensity-coded: dark=pupil, mid=iris, bright=bg/sclera\n",
    "        new_mask[mask > 200] = 0   # bright = background/sclera\n",
    "        new_mask[(mask > 50) & (mask <= 200)] = 1  # mid = iris\n",
    "        new_mask[mask <= 50] = 2   # dark = pupil\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "\n",
    "def extract_iris_roi(image, mask, roi_size):\n",
    "    \"\"\"Extract iris ROI crop for the pupil ROI model.\n",
    "\n",
    "    Returns:\n",
    "        roi_img: (roi_size, roi_size, 3) uint8 or None\n",
    "        roi_msk: (roi_size, roi_size) binary uint8 (1=pupil) or None\n",
    "    \"\"\"\n",
    "    iris_mask = (mask == 1) | (mask == 2)\n",
    "    if not np.any(iris_mask):\n",
    "        return None, None\n",
    "\n",
    "    ys, xs = np.where(iris_mask)\n",
    "    cx, cy = int(np.mean(xs)), int(np.mean(ys))\n",
    "\n",
    "    # Estimate iris radius from mask extents\n",
    "    dists = np.sqrt((xs - cx) ** 2 + (ys - cy) ** 2)\n",
    "    ir = int(np.percentile(dists, 90))\n",
    "\n",
    "    # Crop with padding\n",
    "    pad = int(ir * 1.3)\n",
    "    x1 = max(0, cx - pad)\n",
    "    y1 = max(0, cy - pad)\n",
    "    x2 = min(image.shape[1], cx + pad)\n",
    "    y2 = min(image.shape[0], cy + pad)\n",
    "\n",
    "    roi_img = image[y1:y2, x1:x2]\n",
    "    roi_msk = mask[y1:y2, x1:x2]\n",
    "\n",
    "    if roi_img.shape[0] < 10 or roi_img.shape[1] < 10:\n",
    "        return None, None\n",
    "\n",
    "    roi_img = cv2.resize(roi_img, (roi_size, roi_size))\n",
    "    roi_msk = cv2.resize(roi_msk, (roi_size, roi_size), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Binary pupil mask for ROI model\n",
    "    roi_msk_binary = (roi_msk == 2).astype(np.uint8)\n",
    "    return roi_img, roi_msk_binary\n",
    "\n",
    "\n",
    "print(\"Shared utilities loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MOBIUS Dataset Loader\n",
    "# ============================================================\n",
    "\n",
    "def load_mobius_dataset():\n",
    "    \"\"\"Load MOBIUS dataset (phone camera eye images with segmentation masks).\n",
    "\n",
    "    Returns:\n",
    "        full_images:  list of (256,256,3) uint8\n",
    "        full_masks:   list of (256,256) uint8  {0,1,2}\n",
    "        roi_pairs:    list of (roi_img, roi_msk_binary) tuples\n",
    "    \"\"\"\n",
    "    mobius_dir = DATA_DIR / 'mobius'\n",
    "    if not mobius_dir.exists():\n",
    "        print(\"MOBIUS directory not found. Skipping.\")\n",
    "        return [], [], []\n",
    "\n",
    "    # Collect all image files recursively\n",
    "    img_files = sorted(\n",
    "        glob.glob(str(mobius_dir / '**' / '*.png'), recursive=True) +\n",
    "        glob.glob(str(mobius_dir / '**' / '*.jpg'), recursive=True) +\n",
    "        glob.glob(str(mobius_dir / '**' / '*.bmp'), recursive=True)\n",
    "    )\n",
    "\n",
    "    # Filter out mask/label files\n",
    "    img_files = [\n",
    "        f for f in img_files\n",
    "        if not any(kw in f.lower() for kw in ['mask', 'label', 'seg', 'gt', 'annot'])\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(img_files)} MOBIUS candidate image files\")\n",
    "\n",
    "    full_images, full_masks, roi_pairs = [], [], []\n",
    "\n",
    "    for img_path in tqdm(img_files, desc=\"Loading MOBIUS\"):\n",
    "        mask_path = find_mask_path(img_path)\n",
    "        if not mask_path or not os.path.exists(mask_path):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        msk = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None or msk is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize for full model\n",
    "        img_full = cv2.resize(img, (IMG_SIZE_FULL, IMG_SIZE_FULL))\n",
    "        msk_full = cv2.resize(msk, (IMG_SIZE_FULL, IMG_SIZE_FULL),\n",
    "                              interpolation=cv2.INTER_NEAREST)\n",
    "        msk_full = remap_mask(msk_full)\n",
    "\n",
    "        full_images.append(img_full)\n",
    "        full_masks.append(msk_full)\n",
    "\n",
    "        # Extract iris ROI for ROI model\n",
    "        roi_img, roi_msk = extract_iris_roi(img_full, msk_full, IMG_SIZE_ROI)\n",
    "        if roi_img is not None:\n",
    "            roi_pairs.append((roi_img, roi_msk))\n",
    "\n",
    "    print(f\"Loaded {len(full_images)} MOBIUS samples \"\n",
    "          f\"({len(roi_pairs)} ROI crops)\")\n",
    "    return full_images, full_masks, roi_pairs\n",
    "\n",
    "\n",
    "mobius_imgs, mobius_masks, mobius_rois = load_mobius_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# iBUG Eye Segmentation Loader\n",
    "# ============================================================\n",
    "\n",
    "def load_ibug_dataset():\n",
    "    \"\"\"Load iBUG sclera/eye segmentation dataset.\n",
    "\n",
    "    The iBUG dataset provides sclera segmentation masks.\n",
    "    We derive iris and pupil regions from the non-sclera eye area.\n",
    "\n",
    "    Returns:\n",
    "        full_images, full_masks, roi_pairs\n",
    "    \"\"\"\n",
    "    ibug_dir = DATA_DIR / 'ibug'\n",
    "    if not ibug_dir.exists():\n",
    "        print(\"iBUG directory not found. Skipping.\")\n",
    "        return [], [], []\n",
    "\n",
    "    img_files = sorted(\n",
    "        glob.glob(str(ibug_dir / '**' / '*.png'), recursive=True) +\n",
    "        glob.glob(str(ibug_dir / '**' / '*.jpg'), recursive=True) +\n",
    "        glob.glob(str(ibug_dir / '**' / '*.bmp'), recursive=True)\n",
    "    )\n",
    "\n",
    "    img_files = [\n",
    "        f for f in img_files\n",
    "        if not any(kw in f.lower() for kw in ['mask', 'label', 'seg', 'gt', 'annot'])\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(img_files)} iBUG candidate image files\")\n",
    "\n",
    "    full_images, full_masks, roi_pairs = [], [], []\n",
    "\n",
    "    for img_path in tqdm(img_files, desc=\"Loading iBUG\"):\n",
    "        mask_path = find_mask_path(img_path)\n",
    "        if not mask_path or not os.path.exists(mask_path):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        msk = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None or msk is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img_full = cv2.resize(img, (IMG_SIZE_FULL, IMG_SIZE_FULL))\n",
    "        msk_full = cv2.resize(msk, (IMG_SIZE_FULL, IMG_SIZE_FULL),\n",
    "                              interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # iBUG provides sclera masks - remap to our classes\n",
    "        # Common encoding: sclera=white(255), iris/pupil=other values\n",
    "        msk_full = remap_mask(msk_full)\n",
    "\n",
    "        full_images.append(img_full)\n",
    "        full_masks.append(msk_full)\n",
    "\n",
    "        roi_img, roi_msk = extract_iris_roi(img_full, msk_full, IMG_SIZE_ROI)\n",
    "        if roi_img is not None:\n",
    "            roi_pairs.append((roi_img, roi_msk))\n",
    "\n",
    "    print(f\"Loaded {len(full_images)} iBUG samples \"\n",
    "          f\"({len(roi_pairs)} ROI crops)\")\n",
    "    return full_images, full_masks, roi_pairs\n",
    "\n",
    "\n",
    "ibug_imgs, ibug_masks, ibug_rois = load_ibug_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Roboflow pupilX Loader\n",
    "# ============================================================\n",
    "\n",
    "def load_roboflow_dataset():\n",
    "    \"\"\"Load Roboflow pupilX dataset (iris + pupil instance segmentation).\n",
    "\n",
    "    Supports both API-downloaded and manually-placed data.\n",
    "    Expected directory structure:\n",
    "        data/roboflow/\n",
    "            train/images/  train/masks/\n",
    "            valid/images/  valid/masks/\n",
    "            test/images/   test/masks/\n",
    "    OR flat:\n",
    "        data/roboflow/images/  data/roboflow/masks/\n",
    "\n",
    "    Returns:\n",
    "        full_images, full_masks, roi_pairs\n",
    "    \"\"\"\n",
    "    rb_dir = DATA_DIR / 'roboflow'\n",
    "    if not rb_dir.exists():\n",
    "        print(\"Roboflow directory not found. Skipping.\")\n",
    "        return [], [], []\n",
    "\n",
    "    # Attempt API download if roboflow package is available and no images exist\n",
    "    existing = glob.glob(str(rb_dir / '**' / '*.jpg'), recursive=True) + \\\n",
    "               glob.glob(str(rb_dir / '**' / '*.png'), recursive=True)\n",
    "\n",
    "    if len(existing) == 0:\n",
    "        try:\n",
    "            from roboflow import Roboflow\n",
    "            api_key = os.environ.get('ROBOFLOW_API_KEY', '')\n",
    "            if api_key:\n",
    "                print(\"Downloading pupilX dataset via Roboflow API...\")\n",
    "                rf = Roboflow(api_key=api_key)\n",
    "                project = rf.workspace().project(\"pupilx\")\n",
    "                dataset = project.version(1).download(\n",
    "                    \"png-mask-semantic\", location=str(rb_dir)\n",
    "                )\n",
    "                print(\"Roboflow download complete.\")\n",
    "            else:\n",
    "                print(\"Set ROBOFLOW_API_KEY env var to auto-download, \"\n",
    "                      \"or place data manually in data/roboflow/\")\n",
    "                return [], [], []\n",
    "        except ImportError:\n",
    "            print(\"roboflow package not installed. \"\n",
    "                  \"Place data manually in data/roboflow/\")\n",
    "            return [], [], []\n",
    "\n",
    "    # Collect images from all splits\n",
    "    img_files = sorted(\n",
    "        glob.glob(str(rb_dir / '**' / 'images' / '*.jpg'), recursive=True) +\n",
    "        glob.glob(str(rb_dir / '**' / 'images' / '*.png'), recursive=True)\n",
    "    )\n",
    "\n",
    "    if not img_files:\n",
    "        # Try flat layout\n",
    "        all_files = sorted(\n",
    "            glob.glob(str(rb_dir / '**' / '*.jpg'), recursive=True) +\n",
    "            glob.glob(str(rb_dir / '**' / '*.png'), recursive=True)\n",
    "        )\n",
    "        img_files = [\n",
    "            f for f in all_files\n",
    "            if not any(kw in f.lower() for kw in ['mask', 'label', 'seg'])\n",
    "        ]\n",
    "\n",
    "    print(f\"Found {len(img_files)} Roboflow candidate image files\")\n",
    "\n",
    "    full_images, full_masks, roi_pairs = [], [], []\n",
    "\n",
    "    for img_path in tqdm(img_files, desc=\"Loading Roboflow\"):\n",
    "        mask_path = find_mask_path(img_path)\n",
    "        if not mask_path or not os.path.exists(mask_path):\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        msk = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None or msk is None:\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img_full = cv2.resize(img, (IMG_SIZE_FULL, IMG_SIZE_FULL))\n",
    "        msk_full = cv2.resize(msk, (IMG_SIZE_FULL, IMG_SIZE_FULL),\n",
    "                              interpolation=cv2.INTER_NEAREST)\n",
    "        msk_full = remap_mask(msk_full)\n",
    "\n",
    "        full_images.append(img_full)\n",
    "        full_masks.append(msk_full)\n",
    "\n",
    "        roi_img, roi_msk = extract_iris_roi(img_full, msk_full, IMG_SIZE_ROI)\n",
    "        if roi_img is not None:\n",
    "            roi_pairs.append((roi_img, roi_msk))\n",
    "\n",
    "    print(f\"Loaded {len(full_images)} Roboflow samples \"\n",
    "          f\"({len(roi_pairs)} ROI crops)\")\n",
    "    return full_images, full_masks, roi_pairs\n",
    "\n",
    "\n",
    "roboflow_imgs, roboflow_masks, roboflow_rois = load_roboflow_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Enhanced Synthetic Data Generation\n",
    "\n",
    "Generates realistic synthetic eye images with:\n",
    "- Curved eyelid occlusion (Bezier-style)\n",
    "- Specular highlights (Purkinje images)\n",
    "- Color cast and white-balance variations\n",
    "- Motion blur and defocus\n",
    "- Realistic iris textures with radial fibers\n",
    "- Off-center pupils for anisocoria simulation\n",
    "\n",
    "Each sample produces **both** a full-image (256x256, 3-class) mask\n",
    "and an iris-ROI crop (128x128, binary pupil mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_eye(img_size=256, pupil_ratio=None, anisocoria=False):\n",
    "    \"\"\"Generate a synthetic eye image with precise segmentation mask.\n",
    "\n",
    "    Returns:\n",
    "        image: RGB uint8 array (img_size, img_size, 3)\n",
    "        mask:  uint8 array (img_size, img_size) with {0:bg, 1:iris, 2:pupil}\n",
    "    \"\"\"\n",
    "    img = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "    mask = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "\n",
    "    center_x = img_size // 2 + np.random.randint(-15, 16)\n",
    "    center_y = img_size // 2 + np.random.randint(-15, 16)\n",
    "    center = (center_x, center_y)\n",
    "\n",
    "    # --- Iris parameters ---\n",
    "    iris_radius = np.random.randint(img_size // 4, img_size // 3)\n",
    "\n",
    "    iris_colors = [\n",
    "        (np.random.randint(40, 90),\n",
    "         np.random.randint(60, 120),\n",
    "         np.random.randint(100, 180)),  # brown\n",
    "        (np.random.randint(140, 200),\n",
    "         np.random.randint(100, 160),\n",
    "         np.random.randint(40, 80)),    # blue\n",
    "        (np.random.randint(60, 120),\n",
    "         np.random.randint(120, 180),\n",
    "         np.random.randint(60, 100)),   # green\n",
    "        (np.random.randint(80, 130),\n",
    "         np.random.randint(100, 150),\n",
    "         np.random.randint(120, 180)),  # hazel\n",
    "        (np.random.randint(120, 160),\n",
    "         np.random.randint(120, 160),\n",
    "         np.random.randint(120, 160)),  # gray\n",
    "    ]\n",
    "    iris_color = iris_colors[np.random.randint(0, len(iris_colors))]\n",
    "\n",
    "    # --- Pupil parameters ---\n",
    "    if pupil_ratio is None:\n",
    "        if anisocoria:\n",
    "            pupil_ratio = np.random.choice([\n",
    "                np.random.uniform(0.15, 0.25),  # very constricted\n",
    "                np.random.uniform(0.60, 0.80),  # very dilated\n",
    "            ])\n",
    "        else:\n",
    "            pupil_ratio = np.random.uniform(0.25, 0.65)\n",
    "\n",
    "    pupil_radius = int(iris_radius * pupil_ratio)\n",
    "\n",
    "    # Off-center pupil (more pronounced for anisocoria)\n",
    "    off_range = 6 if anisocoria else 3\n",
    "    pupil_offset_x = np.random.randint(-off_range, off_range + 1)\n",
    "    pupil_offset_y = np.random.randint(-off_range, off_range + 1)\n",
    "    pupil_center = (center_x + pupil_offset_x, center_y + pupil_offset_y)\n",
    "\n",
    "    # --- Sclera ---\n",
    "    sclera_color = (\n",
    "        np.random.randint(220, 250),\n",
    "        np.random.randint(220, 250),\n",
    "        np.random.randint(225, 255),\n",
    "    )\n",
    "    sclera_w = int(iris_radius * np.random.uniform(1.6, 2.2))\n",
    "    sclera_h = int(iris_radius * np.random.uniform(1.1, 1.4))\n",
    "    cv2.ellipse(img, center, (sclera_w, sclera_h), 0, 0, 360, sclera_color, -1)\n",
    "\n",
    "    # --- Iris with radial gradient + limbal ring ---\n",
    "    for r in range(iris_radius, 0, -1):\n",
    "        t = r / iris_radius\n",
    "        darken = 0.4 + 0.6 * t  # darker limbal ring\n",
    "        c = tuple(int(v * darken) for v in iris_color)\n",
    "        cv2.circle(img, center, r, c, -1)\n",
    "    cv2.circle(mask, center, iris_radius, 1, -1)\n",
    "\n",
    "    # --- Iris texture (radial fibers + collarette) ---\n",
    "    num_fibers = np.random.randint(30, 60)\n",
    "    for _ in range(num_fibers):\n",
    "        angle = np.random.uniform(0, 2 * np.pi)\n",
    "        r_start = pupil_radius * np.random.uniform(1.05, 1.2)\n",
    "        r_end = iris_radius * np.random.uniform(0.5, 0.95)\n",
    "        x1 = int(center_x + np.cos(angle) * r_start)\n",
    "        y1 = int(center_y + np.sin(angle) * r_start)\n",
    "        x2 = int(center_x + np.cos(angle) * r_end)\n",
    "        y2 = int(center_y + np.sin(angle) * r_end)\n",
    "        fc = tuple(max(0, min(255, int(v * np.random.uniform(0.6, 1.4))))\n",
    "                   for v in iris_color)\n",
    "        cv2.line(img, (x1, y1), (x2, y2), fc, 1)\n",
    "\n",
    "    # Collarette ring (mid-iris ring)\n",
    "    collarette_r = int(iris_radius * np.random.uniform(0.45, 0.55))\n",
    "    col_color = tuple(max(0, min(255, int(v * 1.2))) for v in iris_color)\n",
    "    cv2.circle(img, center, collarette_r, col_color, 1)\n",
    "\n",
    "    # --- Pupil ---\n",
    "    pupil_darkness = np.random.randint(3, 25)\n",
    "    cv2.circle(img, pupil_center, pupil_radius,\n",
    "               (pupil_darkness, pupil_darkness, pupil_darkness), -1)\n",
    "    cv2.circle(mask, pupil_center, pupil_radius, 2, -1)\n",
    "\n",
    "    # --- Specular highlights (Purkinje images) ---\n",
    "    num_reflections = np.random.randint(1, 4)\n",
    "    for _ in range(num_reflections):\n",
    "        ref_x = center_x + np.random.randint(-iris_radius // 2, iris_radius // 2)\n",
    "        ref_y = center_y + np.random.randint(-iris_radius // 3, iris_radius // 4)\n",
    "        ref_r = np.random.randint(2, max(3, int(iris_radius * 0.08)))\n",
    "        brightness = np.random.randint(220, 256)\n",
    "        cv2.circle(img, (ref_x, ref_y), ref_r,\n",
    "                   (brightness, brightness, min(255, brightness + 10)), -1)\n",
    "        # Soft glow around highlight\n",
    "        cv2.circle(img, (ref_x, ref_y), ref_r + 2,\n",
    "                   (brightness // 2, brightness // 2, brightness // 2), 1)\n",
    "\n",
    "    # --- Curved eyelids (Bezier-style using polylines) ---\n",
    "    if np.random.random() > 0.2:\n",
    "        skin_color = (\n",
    "            np.random.randint(140, 220),\n",
    "            np.random.randint(120, 200),\n",
    "            np.random.randint(100, 180),\n",
    "        )\n",
    "\n",
    "        # Top eyelid - curved\n",
    "        lid_drop_top = np.random.uniform(0.6, 1.3) * iris_radius\n",
    "        lid_top_y = int(center_y - lid_drop_top)\n",
    "        lid_curve_top = np.random.randint(10, 35)\n",
    "        n_pts = 30\n",
    "        xs_top = np.linspace(0, img_size, n_pts).astype(int)\n",
    "        # Parabolic curve for upper lid\n",
    "        ys_top = (lid_top_y\n",
    "                  - lid_curve_top\n",
    "                  * np.sin(np.linspace(0, np.pi, n_pts))).astype(int)\n",
    "        pts_top = np.column_stack([xs_top, ys_top])\n",
    "        pts_top = np.vstack([[img_size, 0], [0, 0], pts_top])\n",
    "        cv2.fillPoly(img, [pts_top.astype(np.int32)], skin_color)\n",
    "        cv2.fillPoly(mask, [pts_top.astype(np.int32)], 0)\n",
    "        # Eyelash line\n",
    "        for k in range(len(xs_top) - 1):\n",
    "            lash_color = tuple(max(0, c - 60) for c in skin_color)\n",
    "            cv2.line(img, (xs_top[k], ys_top[k]),\n",
    "                     (xs_top[k + 1], ys_top[k + 1]), lash_color, 2)\n",
    "\n",
    "        # Bottom eyelid - curved\n",
    "        lid_drop_bot = np.random.uniform(0.6, 1.3) * iris_radius\n",
    "        lid_bot_y = int(center_y + lid_drop_bot)\n",
    "        lid_curve_bot = np.random.randint(5, 20)\n",
    "        xs_bot = np.linspace(0, img_size, n_pts).astype(int)\n",
    "        ys_bot = (lid_bot_y\n",
    "                  + lid_curve_bot\n",
    "                  * np.sin(np.linspace(0, np.pi, n_pts))).astype(int)\n",
    "        pts_bot = np.column_stack([xs_bot, ys_bot])\n",
    "        pts_bot = np.vstack([pts_bot, [img_size, img_size], [0, img_size]])\n",
    "        cv2.fillPoly(img, [pts_bot.astype(np.int32)], skin_color)\n",
    "        cv2.fillPoly(mask, [pts_bot.astype(np.int32)], 0)\n",
    "\n",
    "    # --- Color cast / white-balance variation ---\n",
    "    if np.random.random() > 0.5:\n",
    "        cast = np.array([\n",
    "            np.random.uniform(-15, 15),\n",
    "            np.random.uniform(-10, 10),\n",
    "            np.random.uniform(-15, 15),\n",
    "        ])\n",
    "        img = np.clip(img.astype(np.float32) + cast, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # --- Noise ---\n",
    "    noise_sigma = np.random.uniform(3, 15)\n",
    "    noise = np.random.normal(0, noise_sigma, img.shape).astype(np.int16)\n",
    "    img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # --- Random brightness / contrast ---\n",
    "    alpha = np.random.uniform(0.7, 1.3)\n",
    "    beta = np.random.randint(-30, 31)\n",
    "    img = np.clip(alpha * img.astype(np.float32) + beta, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # --- Motion blur (occasional) ---\n",
    "    if np.random.random() > 0.8:\n",
    "        ksize = np.random.choice([3, 5, 7])\n",
    "        kernel = np.zeros((ksize, ksize))\n",
    "        if np.random.random() > 0.5:\n",
    "            kernel[ksize // 2, :] = 1.0 / ksize   # horizontal\n",
    "        else:\n",
    "            kernel[:, ksize // 2] = 1.0 / ksize   # vertical\n",
    "        img = cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "# Preview synthetic samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i in range(4):\n",
    "    aniso = i >= 2\n",
    "    img, msk = generate_synthetic_eye(IMG_SIZE_FULL, anisocoria=aniso)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"{'Anisocoria' if aniso else 'Normal'} eye\")\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(msk, cmap='viridis', vmin=0, vmax=2)\n",
    "    axes[1, i].set_title('Mask (0=bg, 1=iris, 2=pupil)')\n",
    "    axes[1, i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Generate synthetic dataset\n# ============================================================\n\nif QUICK_MODE:\n    NUM_NORMAL = 400\n    NUM_ANISOCORIA = 100\nelse:\n    NUM_NORMAL = 10000\n    NUM_ANISOCORIA = 5000\n\nprint(f\"Generating {NUM_NORMAL + NUM_ANISOCORIA} synthetic eye images...\")\n\nsyn_full_images, syn_full_masks, syn_roi_pairs = [], [], []\n\n# Normal pupils\nfor _ in tqdm(range(NUM_NORMAL), desc=\"Synthetic normal\"):\n    img, msk = generate_synthetic_eye(IMG_SIZE_FULL)\n    syn_full_images.append(img)\n    syn_full_masks.append(msk)\n    roi_img, roi_msk = extract_iris_roi(img, msk, IMG_SIZE_ROI)\n    if roi_img is not None:\n        syn_roi_pairs.append((roi_img, roi_msk))\n\n# Anisocoria cases\nfor _ in tqdm(range(NUM_ANISOCORIA), desc=\"Synthetic anisocoria\"):\n    img, msk = generate_synthetic_eye(IMG_SIZE_FULL, anisocoria=True)\n    syn_full_images.append(img)\n    syn_full_masks.append(msk)\n    roi_img, roi_msk = extract_iris_roi(img, msk, IMG_SIZE_ROI)\n    if roi_img is not None:\n        syn_roi_pairs.append((roi_img, roi_msk))\n\nprint(f\"\\nSynthetic: {len(syn_full_images)} full images, \"\n      f\"{len(syn_roi_pairs)} ROI crops\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Merge all datasets and create train/val splits\n",
    "# ============================================================\n",
    "\n",
    "# --- Full image model data (256x256, 3-class) ---\n",
    "all_full_images = (mobius_imgs + ibug_imgs + roboflow_imgs + syn_full_images)\n",
    "all_full_masks  = (mobius_masks + ibug_masks + roboflow_masks + syn_full_masks)\n",
    "\n",
    "# --- ROI model data (128x128, binary) ---\n",
    "all_roi_pairs = (mobius_rois + ibug_rois + roboflow_rois + syn_roi_pairs)\n",
    "\n",
    "print(f\"Dataset summary:\")\n",
    "print(f\"  MOBIUS    : {len(mobius_imgs):>6} full / {len(mobius_rois):>6} ROI\")\n",
    "print(f\"  iBUG      : {len(ibug_imgs):>6} full / {len(ibug_rois):>6} ROI\")\n",
    "print(f\"  Roboflow  : {len(roboflow_imgs):>6} full / {len(roboflow_rois):>6} ROI\")\n",
    "print(f\"  Synthetic : {len(syn_full_images):>6} full / {len(syn_roi_pairs):>6} ROI\")\n",
    "print(f\"  \" + \"-\" * 40)\n",
    "print(f\"  TOTAL     : {len(all_full_images):>6} full / {len(all_roi_pairs):>6} ROI\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "full_images = np.array(all_full_images, dtype=np.uint8)\n",
    "full_masks  = np.array(all_full_masks, dtype=np.uint8)\n",
    "\n",
    "roi_images = np.array([p[0] for p in all_roi_pairs], dtype=np.uint8)\n",
    "roi_masks  = np.array([p[1] for p in all_roi_pairs], dtype=np.uint8)\n",
    "\n",
    "# Train / validation split (85% / 15%)\n",
    "X_full_train, X_full_val, y_full_train, y_full_val = train_test_split(\n",
    "    full_images, full_masks, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "X_roi_train, X_roi_val, y_roi_train, y_roi_val = train_test_split(\n",
    "    roi_images, roi_masks, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nFull model:  train={len(X_full_train)}, val={len(X_full_val)}\")\n",
    "print(f\"ROI  model:  train={len(X_roi_train)}, val={len(X_roi_val)}\")\n",
    "\n",
    "# Free raw lists to save memory\n",
    "del all_full_images, all_full_masks, all_roi_pairs\n",
    "del mobius_imgs, mobius_masks, mobius_rois\n",
    "del ibug_imgs, ibug_masks, ibug_rois\n",
    "del roboflow_imgs, roboflow_masks, roboflow_rois\n",
    "del syn_full_images, syn_full_masks, syn_roi_pairs\n",
    "\n",
    "# Mask class distribution (full model)\n",
    "print(f\"\\nFull mask class distribution (train):\")\n",
    "print(f\"  bg   = {np.mean(y_full_train == 0):.1%}\")\n",
    "print(f\"  iris = {np.mean(y_full_train == 1):.1%}\")\n",
    "print(f\"  pupil= {np.mean(y_full_train == 2):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Augmentation Pipeline\n",
    "\n",
    "Shared augmentation applied on-the-fly via `tf.data` pipelines.\n",
    "Uses `albumentations` for photometric + geometric transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Augmentation definitions (albumentations 2.0+ API)\n# ============================================================\n\naugment_full = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.Affine(\n        scale=(0.85, 1.15), translate_percent=(-0.1, 0.1),\n        rotate=(-15, 15), p=0.7, border_mode=cv2.BORDER_CONSTANT),\n    A.OneOf([\n        A.GaussianBlur(blur_limit=(3, 7), p=1),\n        A.MotionBlur(blur_limit=(3, 7), p=1),\n    ], p=0.3),\n    A.OneOf([\n        A.RandomBrightnessContrast(brightness_limit=0.3,\n                                   contrast_limit=0.3, p=1),\n        A.CLAHE(clip_limit=4.0, p=1),\n    ], p=0.5),\n    A.GaussNoise(p=0.3),\n    A.ColorJitter(brightness=0.2, contrast=0.2,\n                  saturation=0.2, hue=0.05, p=0.4),\n    A.CoarseDropout(num_holes_range=(1, 3),\n                    hole_height_range=(10, 30),\n                    hole_width_range=(10, 30),\n                    fill=0, p=0.2),\n])\n\naugment_roi = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.Affine(\n        scale=(0.88, 1.12), translate_percent=(-0.08, 0.08),\n        rotate=(-10, 10), p=0.6, border_mode=cv2.BORDER_CONSTANT),\n    A.OneOf([\n        A.GaussianBlur(blur_limit=(3, 5), p=1),\n        A.MotionBlur(blur_limit=(3, 5), p=1),\n    ], p=0.25),\n    A.RandomBrightnessContrast(brightness_limit=0.25,\n                               contrast_limit=0.25, p=0.5),\n    A.GaussNoise(p=0.3),\n    A.ColorJitter(brightness=0.15, contrast=0.15,\n                  saturation=0.15, hue=0.04, p=0.3),\n])\n\n\n# ============================================================\n# tf.data pipeline builders\n# ============================================================\n\ndef _make_full_dataset(images, masks, batch_size, aug_fn=None, shuffle=True):\n    \"\"\"Build tf.data pipeline for the 3-class full-image model.\"\"\"\n    def _augment(image, mask):\n        def _apply(img, msk):\n            img_np = (img.numpy() * 255).astype(np.uint8)\n            msk_np = np.argmax(msk.numpy(), axis=-1).astype(np.uint8)\n            result = aug_fn(image=img_np, mask=msk_np)\n            aug_img = result['image'].astype(np.float32) / 255.0\n            aug_msk = tf.keras.utils.to_categorical(\n                result['mask'], NUM_CLASSES).astype(np.float32)\n            return aug_img, aug_msk\n\n        img_a, msk_a = tf.py_function(\n            _apply, [image, mask], [tf.float32, tf.float32])\n        img_a.set_shape(image.shape)\n        msk_a.set_shape(mask.shape)\n        return img_a, msk_a\n\n    imgs_f = images.astype(np.float32) / 255.0\n    msks_oh = tf.keras.utils.to_categorical(masks, NUM_CLASSES).astype(np.float32)\n\n    ds = tf.data.Dataset.from_tensor_slices((imgs_f, msks_oh))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=min(len(images), 4000))\n    if aug_fn is not None:\n        ds = ds.map(_augment, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n\ndef _make_roi_dataset(images, masks, batch_size, aug_fn=None, shuffle=True):\n    \"\"\"Build tf.data pipeline for the binary ROI model.\"\"\"\n    def _augment(image, mask):\n        def _apply(img, msk):\n            img_np = (img.numpy() * 255).astype(np.uint8)\n            msk_np = (msk.numpy()[..., 0] * 255).astype(np.uint8)\n            result = aug_fn(image=img_np, mask=msk_np)\n            aug_img = result['image'].astype(np.float32) / 255.0\n            aug_msk = (result['mask'].astype(np.float32) / 255.0)[..., np.newaxis]\n            return aug_img, aug_msk\n\n        img_a, msk_a = tf.py_function(\n            _apply, [image, mask], [tf.float32, tf.float32])\n        img_a.set_shape(image.shape)\n        msk_a.set_shape(mask.shape)\n        return img_a, msk_a\n\n    imgs_f = images.astype(np.float32) / 255.0\n    msks_f = masks.astype(np.float32)[..., np.newaxis]  # (N, H, W, 1)\n\n    ds = tf.data.Dataset.from_tensor_slices((imgs_f, msks_f))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=min(len(images), 4000))\n    if aug_fn is not None:\n        ds = ds.map(_augment, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n\n# Build datasets\ntrain_full_ds = _make_full_dataset(X_full_train, y_full_train,\n                                   BATCH_SIZE, aug_fn=augment_full)\nval_full_ds   = _make_full_dataset(X_full_val, y_full_val,\n                                   BATCH_SIZE, aug_fn=None, shuffle=False)\n\ntrain_roi_ds = _make_roi_dataset(X_roi_train, y_roi_train,\n                                 BATCH_SIZE, aug_fn=augment_roi)\nval_roi_ds   = _make_roi_dataset(X_roi_val, y_roi_val,\n                                 BATCH_SIZE, aug_fn=None, shuffle=False)\n\nprint(f\"Full  train batches: {tf.data.experimental.cardinality(train_full_ds).numpy()}\")\nprint(f\"Full  val   batches: {tf.data.experimental.cardinality(val_full_ds).numpy()}\")\nprint(f\"ROI   train batches: {tf.data.experimental.cardinality(train_roi_ds).numpy()}\")\nprint(f\"ROI   val   batches: {tf.data.experimental.cardinality(val_roi_ds).numpy()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Model A -- Pupil ROI Model\n",
    "\n",
    "Tiny U-Net for **binary pupil segmentation** within an iris ROI crop.  \n",
    "- Input: 128x128x3 (cropped iris region)  \n",
    "- Output: 128x128x1 sigmoid (pupil probability)  \n",
    "- **No pretrained backbone** -- keeps the TFLite export under 2 MB  \n",
    "- Designed for in-browser inference via TensorFlow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pupil_roi_model(input_shape=(128, 128, 3)):\n",
    "    \"\"\"Tiny U-Net for binary pupil segmentation within iris ROI.\n",
    "    Target: <2 MB TFLite, runs in-browser via TF.js.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # --- Encoder ---\n",
    "    c1 = layers.Conv2D(16, 3, padding='same', activation='relu')(inputs)\n",
    "    c1 = layers.Conv2D(16, 3, padding='same', activation='relu')(c1)\n",
    "    p1 = layers.MaxPooling2D()(c1)  # 64x64\n",
    "\n",
    "    c2 = layers.Conv2D(32, 3, padding='same', activation='relu')(p1)\n",
    "    c2 = layers.Conv2D(32, 3, padding='same', activation='relu')(c2)\n",
    "    p2 = layers.MaxPooling2D()(c2)  # 32x32\n",
    "\n",
    "    c3 = layers.Conv2D(64, 3, padding='same', activation='relu')(p2)\n",
    "    c3 = layers.Conv2D(64, 3, padding='same', activation='relu')(c3)\n",
    "    p3 = layers.MaxPooling2D()(c3)  # 16x16\n",
    "\n",
    "    # --- Bottleneck ---\n",
    "    c4 = layers.Conv2D(128, 3, padding='same', activation='relu')(p3)\n",
    "    c4 = layers.Conv2D(128, 3, padding='same', activation='relu')(c4)\n",
    "\n",
    "    # --- Decoder ---\n",
    "    u3 = layers.UpSampling2D()(c4)  # 32x32\n",
    "    u3 = layers.Concatenate()([u3, c3])\n",
    "    u3 = layers.Conv2D(64, 3, padding='same', activation='relu')(u3)\n",
    "\n",
    "    u2 = layers.UpSampling2D()(u3)  # 64x64\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    u2 = layers.Conv2D(32, 3, padding='same', activation='relu')(u2)\n",
    "\n",
    "    u1 = layers.UpSampling2D()(u2)  # 128x128\n",
    "    u1 = layers.Concatenate()([u1, c1])\n",
    "    u1 = layers.Conv2D(16, 3, padding='same', activation='relu')(u1)\n",
    "\n",
    "    # --- Output: single channel sigmoid (binary mask) ---\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid',\n",
    "                            name='pupil_mask')(u1)\n",
    "\n",
    "    model = Model(inputs, outputs, name='pupil_roi_segnet')\n",
    "    return model\n",
    "\n",
    "\n",
    "roi_model = build_pupil_roi_model()\n",
    "roi_model.summary()\n",
    "\n",
    "# Verify parameter count is small enough for <2 MB TFLite\n",
    "total_params = roi_model.count_params()\n",
    "est_size_mb = total_params * 2 / (1024 * 1024)  # float16\n",
    "print(f\"\\nEstimated TFLite (float16) size: ~{est_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Train Pupil ROI Model -- two-phase training\n# ============================================================\n\n# --- Loss functions for binary segmentation ---\n\ndef binary_dice_coeff(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)\n    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]), tf.float32)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2.0 * intersection + smooth) / (\n        tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\n\ndef binary_dice_loss(y_true, y_pred):\n    return 1.0 - binary_dice_coeff(y_true, y_pred)\n\n\ndef bce_dice_loss(y_true, y_pred):\n    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    bce = tf.reduce_mean(bce)\n    return bce + binary_dice_loss(y_true, y_pred)\n\n\ndef binary_iou(y_true, y_pred, threshold=0.5):\n    y_pred_b = tf.cast(y_pred > threshold, tf.float32)\n    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)\n    y_pred_f = tf.reshape(y_pred_b, [-1])\n    inter = tf.reduce_sum(y_true_f * y_pred_f)\n    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - inter\n    return (inter + 1e-6) / (union + 1e-6)\n\n\nROI_EPOCHS_P1 = 3 if QUICK_MODE else 15\nROI_EPOCHS_P2 = 3 if QUICK_MODE else 15\n\n# --- Phase 1: full training (no frozen layers -- no pretrained backbone) ---\nprint(\"=\" * 60)\nprint(f\"ROI Model -- Phase 1: initial training (lr=1e-3, {ROI_EPOCHS_P1} epochs)\")\nprint(\"=\" * 60)\n\nroi_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss=bce_dice_loss,\n    metrics=[binary_dice_coeff, binary_iou],\n)\n\nroi_callbacks_p1 = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_binary_dice_coeff', patience=5,\n        mode='max', restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_binary_dice_coeff', factor=0.5,\n        patience=3, mode='max', min_lr=1e-5),\n]\n\nroi_hist_p1 = roi_model.fit(\n    train_roi_ds,\n    validation_data=val_roi_ds,\n    epochs=ROI_EPOCHS_P1,\n    callbacks=roi_callbacks_p1,\n)\n\n# --- Phase 2: fine-tune with lower LR ---\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"ROI Model -- Phase 2: fine-tune (lr=1e-4, {ROI_EPOCHS_P2} epochs)\")\nprint(\"=\" * 60)\n\nroi_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss=bce_dice_loss,\n    metrics=[binary_dice_coeff, binary_iou],\n)\n\nroi_callbacks_p2 = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_binary_dice_coeff', patience=7,\n        mode='max', restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_binary_dice_coeff', factor=0.5,\n        patience=3, mode='max', min_lr=1e-6),\n    tf.keras.callbacks.ModelCheckpoint(\n        str(MODELS_DIR / 'roi_best.keras'),\n        monitor='val_binary_dice_coeff',\n        mode='max', save_best_only=True),\n]\n\nroi_hist_p2 = roi_model.fit(\n    train_roi_ds,\n    validation_data=val_roi_ds,\n    epochs=ROI_EPOCHS_P2,\n    callbacks=roi_callbacks_p2,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Model B -- Full Image Model\n",
    "\n",
    "MobileNetV3-Small U-Net for **3-class segmentation** of the full eye image.  \n",
    "- Input: 256x256x3 (full eye image)  \n",
    "- Output: 256x256x3 softmax (background, iris, pupil)  \n",
    "- Encoder: MobileNetV3-Small with ImageNet weights  \n",
    "- Target: ~10-15 MB SavedModel, runs on Cloud Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "def conv_block(x, filters, kernel_size=3):\n    \"\"\"Conv + BatchNorm + ReLU.\"\"\"\n    x = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x\n\n\ndef build_full_image_model(input_shape=(256, 256, 3), num_classes=3):\n    \"\"\"U-Net with MobileNetV3-Small encoder (ImageNet weights).\"\"\"\n\n    base_model = tf.keras.applications.MobileNetV3Small(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet',\n        minimalistic=True,\n    )\n\n    # Build a test input to get output shapes (Keras 3 compatible)\n    test_input = tf.keras.Input(shape=input_shape)\n    _ = base_model(test_input)\n\n    # Find skip-connection layers at various spatial resolutions\n    target_sizes = [128, 64, 32, 16, 8]\n    skip_layers = []\n    for target in target_sizes:\n        for layer in reversed(base_model.layers):\n            try:\n                out_shape = layer.output.shape\n            except (AttributeError, RuntimeError):\n                continue\n            if len(out_shape) == 4 and out_shape[1] == target:\n                if layer.name not in skip_layers:\n                    skip_layers.append(layer.name)\n                    break\n\n    print(f\"Skip connection layers: {skip_layers}\")\n\n    encoder_outputs = [base_model.get_layer(n).output for n in skip_layers]\n    encoder = Model(inputs=base_model.input, outputs=encoder_outputs)\n\n    # Freeze encoder for phase 1\n    encoder.trainable = False\n\n    # Decoder\n    inputs = layers.Input(shape=input_shape)\n    skips = encoder(inputs)\n\n    # Handle case where encoder has single output (not a list)\n    if not isinstance(skips, list):\n        skips = [skips]\n\n    x = skips[-1]  # deepest features\n\n    decoder_filters = [128, 64, 48, 32, 24]\n    for i in range(len(skips) - 2, -1, -1):\n        x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n        skip = skips[i]\n        if x.shape[1] != skip.shape[1] or x.shape[2] != skip.shape[2]:\n            x = layers.Resizing(skip.shape[1], skip.shape[2])(x)\n        x = layers.Concatenate()([x, skip])\n        f = decoder_filters[min(i, len(decoder_filters) - 1)]\n        x = conv_block(x, f)\n        x = conv_block(x, f)\n\n    # Upsample to input resolution\n    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n    if x.shape[1] != input_shape[0]:\n        x = layers.Resizing(input_shape[0], input_shape[1])(x)\n    x = conv_block(x, 16)\n\n    outputs = layers.Conv2D(num_classes, 1, activation='softmax',\n                            name='segmentation')(x)\n\n    model = Model(inputs=inputs, outputs=outputs,\n                  name='full_image_segnet')\n    return model, encoder\n\n\nfull_model, full_encoder = build_full_image_model()\nfull_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Train Full Image Model -- two-phase training\n# ============================================================\n\n# --- Loss functions for multi-class segmentation ---\n\ndef dice_coefficient(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.cast(tf.reshape(y_true, [-1, NUM_CLASSES]), tf.float32)\n    y_pred_f = tf.cast(tf.reshape(y_pred, [-1, NUM_CLASSES]), tf.float32)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n    dice = (2.0 * intersection + smooth) / (union + smooth)\n    return tf.reduce_mean(dice[1:])  # exclude background\n\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coefficient(y_true, y_pred)\n\n\ndef combined_loss(y_true, y_pred):\n    ce = tf.reduce_mean(\n        tf.keras.losses.categorical_crossentropy(y_true, y_pred))\n    return ce + dice_loss(y_true, y_pred)\n\n\ndef pupil_iou(y_true, y_pred):\n    y_true_p = y_true[..., 2]\n    y_pred_p = tf.cast(tf.argmax(y_pred, axis=-1) == 2, tf.float32)\n    inter = tf.reduce_sum(y_true_p * y_pred_p)\n    union = tf.reduce_sum(y_true_p) + tf.reduce_sum(y_pred_p) - inter\n    return (inter + 1e-6) / (union + 1e-6)\n\n\ndef iris_iou(y_true, y_pred):\n    y_true_i = y_true[..., 1]\n    y_pred_i = tf.cast(tf.argmax(y_pred, axis=-1) == 1, tf.float32)\n    inter = tf.reduce_sum(y_true_i * y_pred_i)\n    union = tf.reduce_sum(y_true_i) + tf.reduce_sum(y_pred_i) - inter\n    return (inter + 1e-6) / (union + 1e-6)\n\n\nFULL_EPOCHS_P1 = 3 if QUICK_MODE else 10\nFULL_EPOCHS_P2 = 3 if QUICK_MODE else 20\n\n# ---- Phase 1: frozen encoder ----\nprint(\"=\" * 60)\nprint(f\"Full Model -- Phase 1: frozen encoder (lr=1e-3, {FULL_EPOCHS_P1} epochs)\")\nprint(\"=\" * 60)\n\nfull_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss=combined_loss,\n    metrics=[dice_coefficient, pupil_iou, iris_iou],\n)\n\nfull_cbs_p1 = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_dice_coefficient', patience=5,\n        mode='max', restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_dice_coefficient', factor=0.5,\n        patience=3, mode='max', min_lr=1e-5),\n]\n\nfull_hist_p1 = full_model.fit(\n    train_full_ds,\n    validation_data=val_full_ds,\n    epochs=FULL_EPOCHS_P1,\n    callbacks=full_cbs_p1,\n)\n\n# ---- Phase 2: fine-tune entire model ----\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"Full Model -- Phase 2: fine-tune all (lr=1e-4, {FULL_EPOCHS_P2} epochs)\")\nprint(\"=\" * 60)\n\nfull_encoder.trainable = True\n\nfull_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    loss=combined_loss,\n    metrics=[dice_coefficient, pupil_iou, iris_iou],\n)\n\nfull_cbs_p2 = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_dice_coefficient', patience=7,\n        mode='max', restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_dice_coefficient', factor=0.5,\n        patience=3, mode='max', min_lr=1e-6),\n    tf.keras.callbacks.ModelCheckpoint(\n        str(MODELS_DIR / 'full_best.keras'),\n        monitor='val_dice_coefficient',\n        mode='max', save_best_only=True),\n]\n\nfull_hist_p2 = full_model.fit(\n    train_full_ds,\n    validation_data=val_full_ds,\n    epochs=FULL_EPOCHS_P2,\n    callbacks=full_cbs_p2,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Training visualization\n",
    "# ============================================================\n",
    "\n",
    "def _plot_two_phase(h1, h2, metrics, titles, suptitle):\n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 5))\n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, metric, title in zip(axes, metrics, titles):\n",
    "        train_v = h1.history.get(metric, []) + h2.history.get(metric, [])\n",
    "        val_v   = h1.history.get(f'val_{metric}', []) + \\\n",
    "                  h2.history.get(f'val_{metric}', [])\n",
    "        ep = range(1, len(train_v) + 1)\n",
    "        ax.plot(ep, train_v, 'b-', label='Train')\n",
    "        ax.plot(ep, val_v, 'r-', label='Val')\n",
    "        ax.axvline(x=len(h1.history.get(metric, [])),\n",
    "                   color='g', ls='--', label='Phase 2 start')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    fig.suptitle(suptitle, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ROI model curves\n",
    "_plot_two_phase(\n",
    "    roi_hist_p1, roi_hist_p2,\n",
    "    ['loss', 'binary_dice_coeff', 'binary_iou'],\n",
    "    ['Loss', 'Dice', 'IoU'],\n",
    "    'Pupil ROI Model Training',\n",
    ")\n",
    "\n",
    "# Full model curves\n",
    "_plot_two_phase(\n",
    "    full_hist_p1, full_hist_p2,\n",
    "    ['loss', 'dice_coefficient', 'pupil_iou', 'iris_iou'],\n",
    "    ['Loss', 'Dice', 'Pupil IoU', 'Iris IoU'],\n",
    "    'Full Image Model Training',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Compute IoU, Dice, and circle-fitting accuracy for both models on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Evaluate both models\n# ============================================================\n\n# --- ROI model ---\nprint(\"Pupil ROI Model -- Validation\")\nroi_results = roi_model.evaluate(val_roi_ds)\nprint(f\"  Loss : {roi_results[0]:.4f}\")\nprint(f\"  Dice : {roi_results[1]:.4f}\")\nprint(f\"  IoU  : {roi_results[2]:.4f}\")\n\n# --- Full model ---\nprint(\"\\nFull Image Model -- Validation\")\nfull_results = full_model.evaluate(val_full_ds)\nprint(f\"  Loss      : {full_results[0]:.4f}\")\nprint(f\"  Dice      : {full_results[1]:.4f}\")\nprint(f\"  Pupil IoU : {full_results[2]:.4f}\")\nprint(f\"  Iris IoU  : {full_results[3]:.4f}\")\n\n\n# --- Circle-fitting accuracy (full model) ---\ndef fit_circle_from_mask(mask_2d, class_id):\n    \"\"\"Fit a circle to a segmentation mask region via contour moments.\"\"\"\n    binary = (mask_2d == class_id).astype(np.uint8)\n    contours, _ = cv2.findContours(\n        binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not contours:\n        return None\n    largest = max(contours, key=cv2.contourArea)\n    if len(largest) < 5:\n        return None\n    M = cv2.moments(largest)\n    if M['m00'] == 0:\n        return None\n    cx = M['m10'] / M['m00']\n    cy = M['m01'] / M['m00']\n    area = cv2.contourArea(largest)\n    radius = np.sqrt(area / np.pi)\n    return {'x': cx, 'y': cy, 'radius': radius}\n\n\nn_circle_eval = 20 if QUICK_MODE else 200\nprint(f\"\\nCircle-fitting accuracy on {n_circle_eval} validation samples...\")\ncenter_errors, radius_errors = [], []\nn_eval = min(n_circle_eval, len(X_full_val))\nfor i in tqdm(range(n_eval), desc=\"Circle fit\"):\n    img = X_full_val[i].astype(np.float32) / 255.0\n    gt_mask = y_full_val[i]\n    pred = full_model.predict(img[np.newaxis, ...], verbose=0)[0]\n    pred_mask = np.argmax(pred, axis=-1).astype(np.uint8)\n\n    gt_circle = fit_circle_from_mask(gt_mask, 2)\n    pr_circle = fit_circle_from_mask(pred_mask, 2)\n\n    if gt_circle and pr_circle:\n        ce = np.sqrt((gt_circle['x'] - pr_circle['x']) ** 2 +\n                     (gt_circle['y'] - pr_circle['y']) ** 2)\n        re = abs(gt_circle['radius'] - pr_circle['radius'])\n        center_errors.append(ce)\n        radius_errors.append(re)\n\nif center_errors:\n    print(f\"  Pupil center error : {np.mean(center_errors):.2f} px \"\n          f\"(median {np.median(center_errors):.2f})\")\n    print(f\"  Pupil radius error : {np.mean(radius_errors):.2f} px \"\n          f\"(median {np.median(radius_errors):.2f})\")\nelse:\n    print(\"  Could not evaluate circle fitting (no valid detections).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize predictions side-by-side for both models\n",
    "# ============================================================\n",
    "\n",
    "def visualize_both_models(roi_model, full_model, n=4):\n",
    "    fig, axes = plt.subplots(n, 5, figsize=(22, 4.5 * n))\n",
    "    col_titles = ['Full Input', 'GT (3-class)', 'Full Pred',\n",
    "                  'ROI Input', 'ROI Pred (pupil)']\n",
    "\n",
    "    for i in range(n):\n",
    "        idx_full = np.random.randint(len(X_full_val))\n",
    "        idx_roi  = np.random.randint(len(X_roi_val))\n",
    "\n",
    "        # Full model\n",
    "        img_f = X_full_val[idx_full].astype(np.float32) / 255.0\n",
    "        gt_f  = y_full_val[idx_full]\n",
    "        pred_f = full_model.predict(img_f[np.newaxis, ...], verbose=0)[0]\n",
    "        pred_f_class = np.argmax(pred_f, axis=-1)\n",
    "\n",
    "        axes[i, 0].imshow(img_f)\n",
    "        axes[i, 0].set_title(col_titles[0])\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(gt_f, cmap='viridis', vmin=0, vmax=2)\n",
    "        axes[i, 1].set_title(col_titles[1])\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(pred_f_class, cmap='viridis', vmin=0, vmax=2)\n",
    "        axes[i, 2].set_title(col_titles[2])\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "        # ROI model\n",
    "        img_r = X_roi_val[idx_roi].astype(np.float32) / 255.0\n",
    "        pred_r = roi_model.predict(img_r[np.newaxis, ...], verbose=0)[0][..., 0]\n",
    "\n",
    "        axes[i, 3].imshow(img_r)\n",
    "        axes[i, 3].set_title(col_titles[3])\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "        axes[i, 4].imshow(pred_r, cmap='hot', vmin=0, vmax=1)\n",
    "        axes[i, 4].set_title(col_titles[4])\n",
    "        axes[i, 4].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_both_models(roi_model, full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "1. **ROI model** -- float16 quantized TFLite for in-browser use (`../models/pupil_segnet.tflite`)\n",
    "2. **Full model** -- TF SavedModel for Cloud Run (`../cloud/model/pupil_segnet/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Export ROI model as float16 quantized TFLite\n",
    "# ============================================================\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(roi_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save to project models directory\n",
    "project_models_dir = Path('..') / 'models'\n",
    "project_models_dir.mkdir(exist_ok=True)\n",
    "tflite_path = project_models_dir / 'pupil_segnet.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "size_mb = len(tflite_model) / (1024 * 1024)\n",
    "print(f\"ROI TFLite model: {size_mb:.1f} MB -> {tflite_path.resolve()}\")\n",
    "if size_mb > 2.0:\n",
    "    print(\"WARNING: TFLite model exceeds 2 MB target!\")\n",
    "else:\n",
    "    print(\"Size is within the <2 MB target.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Export Full model as SavedModel for Cloud Run\n# ============================================================\n\nsaved_model_path = Path('..') / 'cloud' / 'model' / 'pupil_segnet'\nsaved_model_path.parent.mkdir(parents=True, exist_ok=True)\n\n# Keras 3: use model.export() for SavedModel format\nfull_model.export(str(saved_model_path))\n\nprint(f\"Full SavedModel exported to: {saved_model_path.resolve()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Verify both exports\n# ============================================================\n\nprint(\"--- Verifying TFLite (ROI model) ---\")\ninterpreter = tf.lite.Interpreter(model_path=str(tflite_path))\ninterpreter.allocate_tensors()\ninp_det = interpreter.get_input_details()\nout_det = interpreter.get_output_details()\nprint(f\"  Input : {inp_det[0]['shape']}, dtype={inp_det[0]['dtype']}\")\nprint(f\"  Output: {out_det[0]['shape']}, dtype={out_det[0]['dtype']}\")\n\ntest_roi = X_roi_val[0:1].astype(np.float32) / 255.0\ninterpreter.set_tensor(inp_det[0]['index'], test_roi)\ninterpreter.invoke()\ntflite_out = interpreter.get_tensor(out_det[0]['index'])\nprint(f\"  Test output shape: {tflite_out.shape}\")\nprint(f\"  Test output range: [{tflite_out.min():.3f}, {tflite_out.max():.3f}]\")\nprint(\"  TFLite verification: OK\")\n\nprint(\"\\n--- Verifying SavedModel (Full model) ---\")\nloaded = tf.saved_model.load(str(saved_model_path))\n# Keras 3 export uses 'serve' or 'serving_default' signature\nsigs = list(loaded.signatures.keys())\nprint(f\"  Available signatures: {sigs}\")\nsig_key = 'serving_default' if 'serving_default' in sigs else sigs[0]\ninfer = loaded.signatures[sig_key]\ntest_full = tf.constant(X_full_val[0:1].astype(np.float32) / 255.0)\noutput = infer(test_full)\nout_key = list(output.keys())[0]\nsm_out = output[out_key].numpy()\nprint(f\"  Input shape : {test_full.shape}\")\nprint(f\"  Output shape: {sm_out.shape}\")\nprint(f\"  Output key  : {out_key}\")\nprint(\"  SavedModel verification: OK\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# Benchmark inference speed\n# ============================================================\n\nN_BENCH = 10 if QUICK_MODE else 50\n\n# --- ROI Keras model ---\ntest_roi_batch = X_roi_val[0:1].astype(np.float32) / 255.0\ntimes = []\nfor _ in range(N_BENCH):\n    t0 = time.time()\n    _ = roi_model.predict(test_roi_batch, verbose=0)\n    times.append(time.time() - t0)\nprint(f\"ROI  Keras  : {np.mean(times)*1000:.1f} ms  (avg of {N_BENCH} runs)\")\n\n# --- ROI TFLite ---\ntimes = []\nfor _ in range(N_BENCH):\n    t0 = time.time()\n    interpreter.set_tensor(inp_det[0]['index'], test_roi_batch)\n    interpreter.invoke()\n    _ = interpreter.get_tensor(out_det[0]['index'])\n    times.append(time.time() - t0)\nprint(f\"ROI  TFLite : {np.mean(times)*1000:.1f} ms  (avg of {N_BENCH} runs)\")\n\n# --- Full Keras model ---\ntest_full_batch = X_full_val[0:1].astype(np.float32) / 255.0\ntimes = []\nfor _ in range(N_BENCH):\n    t0 = time.time()\n    _ = full_model.predict(test_full_batch, verbose=0)\n    times.append(time.time() - t0)\nprint(f\"Full Keras  : {np.mean(times)*1000:.1f} ms  (avg of {N_BENCH} runs)\")\n\n# --- Full SavedModel ---\ntimes = []\nfor _ in range(N_BENCH):\n    t0 = time.time()\n    _ = infer(tf.constant(test_full_batch))\n    times.append(time.time() - t0)\nprint(f\"Full Saved  : {np.mean(times)*1000:.1f} ms  (avg of {N_BENCH} runs)\")\n\nprint(\"\\nAll benchmarks complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Two models trained:\n",
    "\n",
    "| Model | Input | Output | Format | Target |\n",
    "|-------|-------|--------|--------|--------|\n",
    "| **Pupil ROI** | 128x128 RGB | 128x128x1 sigmoid | TFLite (float16) | In-browser (TF.js) |\n",
    "| **Full Image** | 256x256 RGB | 256x256x3 softmax | SavedModel | Cloud Run |\n",
    "\n",
    "### Exported artifacts:\n",
    "- `../models/pupil_segnet.tflite` -- ROI model for browser (<2 MB)\n",
    "- `../cloud/model/pupil_segnet/` -- Full model SavedModel for serving\n",
    "\n",
    "### Datasets used:\n",
    "- MOBIUS (up to 3,559 real phone-camera images)\n",
    "- iBUG Eye Segmentation (up to ~2K images)\n",
    "- Roboflow pupilX (up to 804 images)\n",
    "- Enhanced synthetic augmentation (15,000 images)\n",
    "\n",
    "### Next steps:\n",
    "1. Place `pupil_segnet.tflite` in the project `models/` directory (already done by export)\n",
    "2. Deploy the SavedModel to Cloud Run (see `cloud/` directory)\n",
    "3. Convert TFLite to TF.js format if needed: `tensorflowjs_converter --input_format=tflite ../models/pupil_segnet.tflite ../models/tfjs/`\n",
    "4. Fine-tune on additional real data as it becomes available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}